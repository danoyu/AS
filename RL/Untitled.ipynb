{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocesser une image du jeu (scale down, monochrome)\n",
    "def prepro(I):\n",
    "    \"\"\" preprocessing 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0,len(r))):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rl(nn.Module):\n",
    "        def __init__(self,input_size,h_size, output_size):\n",
    "            super(Rl,self).__init__()\n",
    "            self.linear1 = nn.Linear(input_size,h_size)\n",
    "            self.linear2 = nn.Linear(h_size,output_size)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self,x):\n",
    "            h = F.relu(self.linear1(x))\n",
    "            logp = self.sigmoid(self.linear2(h))\n",
    "            return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n",
      "<MulBackward0 object at 0x7f7cbee8d710>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grez' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4bc2b5b2cef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrez\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;31m#     print(type(loss[1].data[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grez' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO : bug quand reward = 0 et du training voir karpathy : http://karpathy.github.io/2016/05/31/rl/\n",
    "env = gym.make('Pong-v0') #charge l'environement\n",
    "env.reset()  # reinitialiser le jeu\n",
    "\n",
    "env.action_space # liste des actions possibles\n",
    "env.unwrapped.get_action_meanings() # signification des actions\n",
    "observation, reward, done, info = env.step(1) # joue l'action 1\n",
    "# print(observation,done,info)\n",
    "l = 0\n",
    "r = 0\n",
    "r_prec = 0\n",
    "D = 80*80\n",
    "R_model = Rl(D,200,2)\n",
    "gamma = 0.99\n",
    "l = 0\n",
    "render = False\n",
    "optimizer = optim.SGD(R_model.parameters(),lr=0.001)\n",
    "\n",
    "for num_episode,episode in enumerate(range(5)):\n",
    "    log_P_action = []\n",
    "    rewards = []\n",
    "    reward_sum = 0\n",
    "    prec = None\n",
    "    for t in range(1000):\n",
    "        if render : env.render() # afficher l'etat du jeu\n",
    "        \n",
    "        #prepocessing et copie de l'etat precdu jeu\n",
    "        current = prepro(observation)\n",
    "        x =  current - prec if prec is not None else np.zeros(D)\n",
    "        prec = current\n",
    "        \n",
    "        x = Variable(torch.FloatTensor(x))\n",
    "        #calcul des probas d'action\n",
    "        logP = R_model(x)\n",
    "        #tirage d'une action\n",
    "        action = torch.multinomial(logP.exp(),1)\n",
    "        log_P_action.append(logP[action])\n",
    "        \n",
    "        #0  aller en bas \n",
    "        g_action = 2 if action.data[0] == 0 else 3 \n",
    "#         print(action)\n",
    "        observation, reward, done, info = env.step(g_action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        reward_sum += reward\n",
    "        \n",
    "    discount_reward = discount_rewards(rewards)\n",
    "    discount_reward = ( discount_reward - np.mean(discount_reward)) / np.std(discount_reward)\n",
    "\n",
    "    #calcul du loss\n",
    "    loss = []\n",
    "    for i in range(len(log_P_action)):\n",
    "        loss.append(-log_P_action[i] * discount_reward[i])\n",
    "    optimizer.zero_grad()\n",
    "    print(type(loss[1]))\n",
    "    print(loss[1].grad_fn)\n",
    "    print(grez)\n",
    "#     print(type(loss[1].data[0]))\n",
    "    if math.isnan(loss[1].data):\n",
    "        loss = torch.cat(loss).sum()\n",
    "    else:\n",
    "        loss = torch.cat(loss).sum()\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(l)\n",
    "#en entree du reseau du neurone \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # liste des actions possibles\n",
    "env.unwrapped.get_action_meanings() # signification des actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
