{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import torch\n",
    "\n",
    "def read_data(filename):\n",
    "    z = zipfile.ZipFile(filename, 'r')\n",
    "\n",
    "    lines = []\n",
    "    with z.open(z.namelist()[0]) as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i % 100 == 0:\n",
    "                line = line.decode('utf-8').lower().replace(\"'\", \" \").replace(\".\", \"\").replace(\"?\", \"\")\\\n",
    "                    .replace(\"!\", \"\").replace(\":\", \"\").replace(\";\", \"\")\n",
    "                lines.append(line)\n",
    "            i += 1\n",
    "\n",
    "    z.close()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "class LanguageLoader(object):\n",
    "    def __init__(self, input_path, output_path, vocab_size, max_length):\n",
    "        super(LanguageLoader, self).__init__()\n",
    "\n",
    "        self.vocab_size, self.max_length = vocab_size, max_length\n",
    "\n",
    "        try:\n",
    "            self.input_dict = pickle.load(open(\"data/input_dict.p\", \"rb\"))\n",
    "            self.input_vecs = pickle.load(open(\"data/input_vecs.p\", \"rb\"))\n",
    "            self.input_size = len(self.input_dict)\n",
    "\n",
    "            self.output_dict = pickle.load(open(\"data/output_dict.p\", \"rb\"))\n",
    "            self.output_vecs = pickle.load(open(\"data/output_vecs.p\", \"rb\"))\n",
    "            self.output_size = len(self.output_dict)\n",
    "            print(\"Languages found and loaded.\")\n",
    "        except(IOError):\n",
    "            self.input_dict, self.input_vecs, self.input_size = self.init_language(input_path)\n",
    "            pickle.dump(self.input_dict, open(\"data/input_dict.p\", \"wb\"))\n",
    "            pickle.dump(self.input_vecs, open(\"data/input_vecs.p\", \"wb\"))\n",
    "            print(\"Input language loaded.\")\n",
    "\n",
    "            self.output_dict, self.output_vecs, self.output_size = self.init_language(output_path)\n",
    "            pickle.dump(self.output_dict, open(\"data/output_dict.p\", \"wb\"))\n",
    "            pickle.dump(self.output_vecs, open(\"data/output_vecs.p\", \"wb\"))\n",
    "            print(\"Output language loaded.\")\n",
    "\n",
    "        self.input_vecs, self.output_vecs = self.filter(self.input_vecs, self.output_vecs)\n",
    "\n",
    "    def init_language(self, path):\n",
    "        dictionary = [\"<SOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "\n",
    "        corpus = read_data(path)\n",
    "        #corpus = [\"the negative log likelihood loss\", \"it is useful to train a classification problem with n classes\" \"if provided, the optional argument weights should be a 1D Tensor assigning weight to each of the classes\", \"this is particularly useful when you have an unbalanced training set\"]\n",
    "        words = \" \".join(corpus).split()\n",
    "        mc = Counter(words).most_common(self.vocab_size-3)\n",
    "        dictionary.extend([word for word, _ in mc])\n",
    "        vectors = [[self.vectorize(word, dictionary) for word in sentence.split()] for sentence in corpus]\n",
    "\n",
    "        return dictionary, vectors, len(dictionary)\n",
    "\n",
    "    def sentences(self, amount):\n",
    "        indeces = np.random.choice(len(self.input_vecs), amount)\n",
    "        sentences = [(self.input_vecs[i], self.output_vecs[i]) for i in indeces]\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def sentence_to_vec(self, sentence):\n",
    "        vectors = [self.vectorize(word, self.input_dict) for word in sentence.lower().split()]\n",
    "        return vectors\n",
    "\n",
    "    def vec_to_sentence(self, vectors):\n",
    "        sentence = \" \".join([self.output_dict[vec[0, 0]] for vec in vectors])\n",
    "        return sentence\n",
    "\n",
    "    def vectorize(self, word, list):\n",
    "        vec = torch.LongTensor(1, 1).zero_()\n",
    "        index = 2 if word not in list else list.index(word)\n",
    "        vec[0][0] = index\n",
    "        return vec\n",
    "\n",
    "    def filter(self, input_vecs, output_vecs):\n",
    "        i = 0\n",
    "        for _ in input_vecs:\n",
    "            if len(input_vecs[i]) > self.max_length or len(output_vecs[i]) > self.max_length:\n",
    "                input_vecs.pop(i)\n",
    "                output_vecs.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return input_vecs, output_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=500, hidden_size=1000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden_state = self.gru(embedded, hidden)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def first_hidden(self):\n",
    "        return Variable(torch.FloatTensor(1, 1, self.hidden_size).zero_())\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=500, hidden_size=1000):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, 1)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = Variable(input)\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden_state = self.gru(embedded, hidden)\n",
    "        output = output.view(1, output.size(2))\n",
    "        linear = self.linear(output)\n",
    "        softmax = self.softmax(linear)\n",
    "        return output, softmax, hidden_state\n",
    "    \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_size)\n",
    "        self.decoder = Decoder(output_size)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters())\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters())\n",
    "\n",
    "        sos, eos = torch.LongTensor(1, 1).zero_(), torch.LongTensor(1, 1).zero_()\n",
    "        sos[0, 0], eos[0, 0] = 0, 1\n",
    "\n",
    "        self.sos, self.eos = sos, eos\n",
    "\n",
    "\n",
    "    def train(self, input, target):\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        hidden_state = self.encoder.first_hidden()\n",
    "\n",
    "        # Encoder\n",
    "        for ivec in input:\n",
    "            _, hidden_state = self.encoder.forward(Variable(ivec), hidden_state)\n",
    "\n",
    "        # Decoder\n",
    "        target.insert(0, self.sos)\n",
    "        target.append(self.eos)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(len(target) - 1):\n",
    "            o, softmax, hidden_state = self.decoder.forward(target[i], hidden_state)\n",
    "            print(o)\n",
    "            print(softmax)\n",
    "            print(torch.sum(softmax))\n",
    "            print(gr)\n",
    "            total_loss += self.loss(softmax, Variable(target[i+1][0]))\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        self.decoder_optimizer.step()\n",
    "        self.encoder_optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def eval(self, input):\n",
    "        hidden_state = self.encoder.first_hidden()\n",
    "\n",
    "        # Encoder\n",
    "        for ivec in input:\n",
    "            _, hidden_state = self.encoder.forward(ivec, hidden_state)\n",
    "\n",
    "        outputs = []\n",
    "        output = self.sos\n",
    "        # Decoder\n",
    "        while output is not self.eos:\n",
    "            output, _, hidden_state = self.decoder.forward(output, hidden_state)\n",
    "            outputs += output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages found and loaded.\n",
      "Variable containing:\n",
      "( 0  ,.,.) = \n",
      "  0.0551  0.1808 -0.0945  ...  -0.1549 -0.0182  0.0851\n",
      "[torch.FloatTensor of size 1x1x1000]\n",
      "\n",
      "Variable containing:\n",
      " 0.0633  0.1629  0.1551  ...   0.1290  0.3100  0.2841\n",
      "[torch.FloatTensor of size 1x1000]\n",
      "\n",
      "Variable containing:\n",
      " 0.1535  0.1543  0.1757  0.1811  0.1476  0.1878\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c4fd35b5d64a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-c4fd35b5d64a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-c340cabe8fec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "en_path = 'data/en.zip'\n",
    "fr_path = 'data/fr.zip'\n",
    "\n",
    "max_length = 3\n",
    "num_batches = 10\n",
    "vocab_size = 15\n",
    "\n",
    "def main():\n",
    "    data = LanguageLoader(en_path, fr_path, vocab_size, max_length)\n",
    "    rnn = RNN(data.input_size, data.output_size)\n",
    "\n",
    "    losses = []\n",
    "    iter = 0\n",
    "    for input, target in data.sentences(num_batches):\n",
    "        loss = rnn.train(input, target)\n",
    "        if iter % 100 is 0:\n",
    "            print(loss.data[0])\n",
    "        iter += 1\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
