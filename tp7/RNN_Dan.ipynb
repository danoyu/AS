{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charDataset import *\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#train = torch.load('train_data.tx')\n",
    "# vocab = torch.load('vocab.tx')\n",
    "# vocab = dict(zip(sorted(vocab),range(len(vocab))))\n",
    "# n_words = len(vocab)\n",
    "\n",
    "#c = CharDataset('train_data.tx','vocab.tx',50)\n",
    "\n",
    "#retourne une sequence dans [0] et le reste dans [1]\n",
    "def seq(text,debut,fin):\n",
    "    return text[debut:fin], text[fin:]\n",
    "\n",
    "# retourne une sequence codé dans [0] et le reste dans [1]\n",
    "def code_seq(text,debut,fin,vocab):\n",
    "    s = seq(text,debut,fin)\n",
    "    return char2code(s[0],vocab), char2code(s[1],vocab)\n",
    "\n",
    "text = 'je suis la'\n",
    "\n",
    "\n",
    "def transform_one_hot(digit,n):\n",
    "    y_onehot = torch.FloatTensor(n)\n",
    "    y_onehot.zero_()\n",
    "    y_onehot[digit] = 1\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "def transform_one_hot_sequence(sequence,n):\n",
    "    seq_onehot = torch.FloatTensor(len(sequence),n)\n",
    "    for i in range(len(sequence)):\n",
    "        seq_onehot[i] = transform_one_hot(sequence[i],n)\n",
    "    return seq_onehot\n",
    "\n",
    "#transform_one_hot_sequence(char2code('n vreb br',vocab),n_words)\n",
    "\n",
    "def binarize(target, nb_classes, neg_value=0):\n",
    "    \"\"\"\n",
    "    encode target en one-hot, si neg_value \n",
    "    vaut zéro, ou en -1/1, si neg_value vaut\n",
    "    -1 par exemple\n",
    "    \"\"\"\n",
    "    \n",
    "    target = target.long().cuda()\n",
    "    tar_shape = target.shape\n",
    "    y_onehot = torch.FloatTensor(*tar_shape, nb_classes).cuda()\n",
    "    y_onehot.zero_().add_(neg_value)\n",
    "    return y_onehot.scatter_(len(tar_shape), target.view(*tar_shape, 1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je n'ai pas réussi a codé le RNN. Dans le forward, j'obtiens une somme des proba différentes de 1 mais je n'arrive pas à comprendre pourquoi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__():\n",
    "        self.w = self.linear(input_size,input_size)\n",
    "        self.u = self.linear(input_size,input_size)\n",
    "    def forward(x,h,c):\n",
    "        #oublie \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        tanh = nn.Tanh()\n",
    "        f = w(x) + u(h)\n",
    "        f *= c\n",
    "        f = sigmoid(f)\n",
    "        \n",
    "        #ecrire\n",
    "        i = w(x) + u(h)\n",
    "        c_tmp = tanh(i)\n",
    "        i = sigmoid(i)\n",
    "        c = f * c + i*c_tmp\n",
    "        \n",
    "        #lire\n",
    "        o = sigmoid(w(x)+u(h))\n",
    "        h = o * tanh(c)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class Modele(nn.Module):\n",
    "    def __init__(self,l,batch_size):\n",
    "        super(Modele,self).__init__()\n",
    "        self.l = l\n",
    "        self.batch_size = batch_size\n",
    "        #init h a 0 de dimension d a changer a chaque linear donc 2 param a apprendre w_h et w_x\n",
    "        # Il faut 3 linear: \n",
    "        # - un pour passer de Rv -> Rd\n",
    "        # le 1er converti une lettre (dimension v) dans l'espace latent\n",
    "        # - un pour passer de Rd -> Rd\n",
    "        # le 2e predit la suite dans l'espace latent\n",
    "        # - un pour passer de Rd -> Rv\n",
    "        # le 3e transforme dans v\n",
    "        # on doit faire un logSoftmax pour avoir p(y1 | x_0,h_0) puis tirage puis one_hot\n",
    "        self.encodage = nn.Linear(n_words,self.l, bias=False).cuda()\n",
    "        self.decodage = nn.Linear(self.l,self.l, bias=False).cuda()\n",
    "        self.encodage2 = nn.Linear(self.l, n_words, bias=False).cuda()\n",
    "        self.tanh = nn.Tanh().cuda()\n",
    "        self.softmax = nn.Softmax().cuda()\n",
    "        self.logsoftmax = nn.LogSoftmax().cuda()\n",
    "        self.encode = nn.GRUCell(n_words, l).cuda()\n",
    "        self.encode2 = nn.GRUCell(n_words, l).cuda()\n",
    "        self.decode = nn.Linear(l, n_words).cuda()\n",
    "        \n",
    "    #train step\n",
    "    def forward(self, x, y):\n",
    "        output = []\n",
    "        h = Variable(torch.zeros((batch_size,self.l))).cuda()\n",
    "        \n",
    "        for char in torch.split(x, 1, 1):\n",
    "#             h = self.encode(char.squeeze(), h)\n",
    "            h = self.tanh(self.encodage(char.squeeze()) + self.decodage(h))\n",
    "            \n",
    "        for char in torch.split(y,1,1):\n",
    "            pred = (self.encodage2(h))\n",
    "            pred = self.logsoftmax(pred)\n",
    "#             pred = self.logsoftmax(self.decode(h))\n",
    "            output.append(pred)\n",
    "#             h = self.encode2(target.squeeze(),h)\n",
    "            h = self.tanh(self.encodage(char.squeeze()) + self.decodage(h))\n",
    "        return torch.stack(output,1)\n",
    "\n",
    "\n",
    "    \n",
    "    #test\n",
    "    def predict(self,x,taille):\n",
    "        init_length = x.data.shape[1]\n",
    "        output = []\n",
    "        \n",
    "        h = Variable(torch.zeros(self.l)).cuda()\n",
    "\n",
    "        for i in range(init_length):\n",
    "            char = x.select(1,i)\n",
    "#             h = self.encode(char,h)\n",
    "            h = self.tanh(self.encodage(char.squeeze()) + self.decodage(h))\n",
    "        for p in range(taille):\n",
    "            pred = (self.encodage2(h))\n",
    "            pred = self.logsoftmax(pred)\n",
    "#             pred = self.logsoftmax(self.decode(h))\n",
    "            char = torch.multinomial(torch.exp(pred)).squeeze()\n",
    "            output.append(char.data)\n",
    "            char.data = binarize(char.data, n_words)\n",
    "#             h = self.encode2(char,h)\n",
    "            h = self.tanh(self.encodage(char.squeeze()) + self.decodage(h))\n",
    "        return torch.stack(output).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting off end of data so that the batches/sequences divide evenly\n",
      "Epoch  0\n",
      "Train loss  4.5143513679504395\n",
      "Sample : \u001b[1ms with everyone he just knew all\u001b[0m3a6tsugb6fyozormvwqnp3r3wxvkfihti1vfz anka 2628narasg3cg3wlx z78ayravpcxb3i167qrdzlg1h3shf2ycv5z6b2yhyu2vg7vphwjrscybsablgmpnqb0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/.local/lib/python3.5/site-packages/ipykernel_launcher.py:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/dan/.local/lib/python3.5/site-packages/ipykernel_launcher.py:82: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10\n",
      "Train loss  3.712003231048584\n",
      "Sample : \u001b[1mhim the location of the conferen\u001b[0mhsruegoorerlutnpdhsree tesltis ao jsbagn otoraoi mhteafeewhdtathnolsh0tuocatltote n7nrneerndruwdeemoelattsweaarnbaaao7szje4i6eud\n",
      "\n",
      "\n",
      "Epoch  20\n",
      "Train loss  3.548624038696289\n",
      "Sample : \u001b[1mstephen glass carried out of the\u001b[0mpo  t nos srnihesonash s a  tonkl uee y9u aih ottehd inkrtmo r  h mastatlrpgdrgahe r vfiltlorohtohehlst lngut uotueewot nxpa sgr\n",
      "\n",
      "\n",
      "Epoch  30\n",
      "Train loss  3.4825563430786133\n",
      "Sample : \u001b[1mhael glass was cute cool and pop\u001b[0m dhke n sc  porot tceg h lhfsndwae iomooiwrt whe  oes ciho wti ceogo sep nie  ada qiloa oie f l owpi1 awoih  tsxiap aeh ganis  o\n",
      "\n",
      "\n",
      "Epoch  40\n",
      "Train loss  3.4165468215942383\n",
      "Sample : \u001b[1mred in his support for the cub r\u001b[0mb kee pae nar st ha h ne rncon aoit  ase6oflari liha ae 6sns6ftno ha ttaeoiw tar afdlsn ici afrmkedton he asr eyharanh5netgl  po\n",
      "\n",
      "\n",
      "Epoch  50\n",
      "Train loss  3.346327781677246\n",
      "Sample : \u001b[1mnight glass called kelly and he \u001b[0mfet tw i itoraajmwoo ah2 5desar  if ic dh ticg tod hns lhd aftlenadswmlde  wl9topaleere poud awniw cort be tsarm haee wthed tn m\n",
      "\n",
      "\n",
      "Epoch  60\n",
      "Train loss  3.2920069694519043\n",
      "Sample : \u001b[1mantity of errors in the article \u001b[0mw2mwi nmulsroldsyeioleliae haeu uessose buegblnwu aaao yopracylsih sehnbeebo arensewc snscloo nlgoneimh beg asiee fn lad tr ne s\n",
      "\n",
      "\n",
      "Epoch  70\n",
      "Train loss  3.239483594894409\n",
      "Sample : \u001b[1mse of confidence and security ar\u001b[0meemine llah re lr la oxlnrndsthareaw he 1 wat lyegtleolor are ilate eh4le  olr abt itcns asy lasts th bommdedswf whicals pevehss\n",
      "\n",
      "\n",
      "Epoch  80\n",
      "Train loss  3.199471950531006\n",
      "Sample : \u001b[1mos stardom and sibling rivalryva\u001b[0ms npnlis cshhe dese ross eikhr dewttomfe  oeniuevrtow uhm tafn htxoe haloe tas lwdrik hanpt  ooew r hh spassyg hen vd lo lfsr dh\n",
      "\n",
      "\n",
      "Epoch  90\n",
      "Train loss  3.162269353866577\n",
      "Sample : \u001b[1mearly surprised to see lane who \u001b[0mo2nxd roeolald ue ved c flis hepcgib te sbet nssiip mcpprulk longse aisr rhem nqiesvulwetwenrep ratbsdyp aa eul thees hlour ted \n",
      "\n",
      "\n",
      "Epoch  100\n",
      "Train loss  3.127094030380249\n",
      "Sample : \u001b[1mde on the second friday in may i\u001b[0motwe noh roubkty con hergtuprejnann fuesyer1s8o  oeskd nh topeef9of fpeuue oane se toyie seapr1pdfoverufadyssiowtio  hatxyiosrlm\n",
      "\n",
      "\n",
      "Epoch  110\n",
      "Train loss  3.095715045928955\n",
      "Sample : \u001b[1mthen the curtain abruptly fell h\u001b[0men ciwthit brial putiedpit io 2naly coak ehe ns ytoe ycaus was cadongnit yoautise ol an hererirtos araglbog ans th bertyg tan ib\n",
      "\n",
      "\n",
      "Epoch  120\n",
      "Train loss  3.070075273513794\n",
      "Sample : \u001b[1melentlessness of the young repor\u001b[0ms wnd tow iir snmips ua limnfsadribpeand mans crom ialssiit eoel ieres alil tm the   roclst  vrnctlveswunu gitadt frerhuelad tgu\n",
      "\n",
      "\n",
      "Epoch  130\n",
      "Train loss  3.0445146560668945\n",
      "Sample : \u001b[1m long as he was given definitive\u001b[0mge wcnenr the htod tind fonpronet gnatone or cb c9 hafels efepyete cant lied t19lbuco savboin cheld che tlonalwan gcutl nh mbamd\n",
      "\n",
      "\n",
      "Epoch  140\n",
      "Train loss  3.028388738632202\n",
      "Sample : \u001b[1mrelatively small fictional detai\u001b[0mtt ni t wtee tormly wulga ois iha kidackalfed arera lapithes marnt his sn lage ais aall ote togile s ilmg6 kh d jeed terand tese\n",
      "\n",
      "\n",
      "Epoch  150\n",
      "Train loss  3.0090770721435547\n",
      "Sample : \u001b[1mng the call with forbes digital \u001b[0msa aate toa afrfra th wbdkq dy owdedg thl neute iifol9 gasaminf ro cfricjnledgfrliclasld dinelly liivi oleretenmeqe ate eor uhir\n",
      "\n",
      "\n",
      "Epoch  160\n",
      "Train loss  2.9904189109802246\n",
      "Sample : \u001b[1m california software company but\u001b[0m wisslmnn t f padsed towng tolnmgokpeod coe eaty re va1isiny yfodvis tor elel tseaupgst a lagppartos ifise st apvenss at aos peg\n",
      "\n",
      "\n",
      "Epoch  170\n",
      "Train loss  2.9781270027160645\n",
      "Sample : \u001b[1mok over as editor of the new rep\u001b[0mactoes qshkesee tinrerit ro wipducmmoklsg ortss aadis aug thactincmerse tht feraen uuthp ind mensimj ate ato teaanjpiscvbctdnge \n",
      "\n",
      "\n",
      "Epoch  180\n",
      "Train loss  2.9604899883270264\n",
      "Sample : \u001b[1mng shots from his possible doubt\u001b[0mom vokts fied 4ut unge citilreo thom guas hess damteping thenh sfelitu sstarestiot yod yail bea xrelf ands yoncerg eeicug ss yon\n",
      "\n",
      "\n",
      "Epoch  190\n",
      "Train loss  2.9520130157470703\n",
      "Sample : \u001b[1mtments methods he also ingratiat\u001b[0m mearetokb gspar urberyell mathe ta bot aed cf1ted wars th il ficistznvy fhmair hind anl hes porllitb aa s matergew thivnrtie ti\n",
      "\n",
      "\n",
      "Epoch  200\n",
      "Train loss  2.9411797523498535\n",
      "Sample : \u001b[1mad about doing this said forooha\u001b[0md wions lag if thor gomero ben aoy bedlyk t yfoche renuster didene coon tothe jnco oals g uine soln bocmlely et oceceeiante s0in\n",
      "\n",
      "\n",
      "Epoch  210\n",
      "Train loss  2.9234447479248047\n",
      "Sample : \u001b[1mat the new republic with glass w\u001b[0macd eal oo dobo 2ut te has hiw am laiws hectey ha ye2resterwg dent astelnte auk woon e9 the he  ofmbat romt or beked3loy ne laa \n",
      "\n",
      "\n",
      "Epoch  220\n",
      "Train loss  2.9174389839172363\n",
      "Sample : \u001b[1m as a fact checker at the new re\u001b[0mionguo iam pasd ienuyin radleng mhit resor the ofloucingiey crreiceterken vathonr forreaaelald y inglssedien thagleseo nnaipheri\n",
      "\n",
      "\n",
      "Epoch  230\n",
      "Train loss  2.8975820541381836\n",
      "Sample : \u001b[1mrd article on the center for sci\u001b[0mons icdtor talt wedcravee cavenb tas des orastth ithed ion ou te phtmles wab sed orot6pteanlat govo sist birt icr hale bllake  a\n",
      "\n",
      "\n",
      "Epoch  240\n",
      "Train loss  2.8975048065185547\n",
      "Sample : \u001b[1mfort to minimize damage to himse\u001b[0mnpander phee femrked jedrbofnc lass atiutleu wge s feectrereit asicrepwitheveur ffahe me thinves alt gpatse vint romhe towacveit\n",
      "\n",
      "\n",
      "Epoch  250\n",
      "Train loss  2.884047269821167\n",
      "Sample : \u001b[1m jungle april 7 1997 spring brea\u001b[0mngass ha se nta of ailes mhe reang anvulktone noas 7utenunn he57msascin and pa jevithe svere hambrenk mtour as freondas wbosmhan\n",
      "\n",
      "\n",
      "Epoch  260\n",
      "Train loss  2.8784916400909424\n",
      "Sample : \u001b[1ms protg but in one of the incide\u001b[0m em the deouad far weuilony toonly chy saptore bermerton tomd iod warmeithat asrofadperithim he woar ef the pentesesdacro0egindy\n",
      "\n",
      "\n",
      "Epoch  270\n",
      "Train loss  2.871326446533203\n",
      "Sample : \u001b[1mernmental subsidies for cheese a\u001b[0mfr ared codwitilng iwpssm ntw the d me ltengebeh laiwed onbasd gedgeljencand less it the somlicgtave tast easlote st ios lerecin\n",
      "\n",
      "\n",
      "Epoch  280\n",
      "Train loss  2.8656020164489746\n",
      "Sample : \u001b[1m being replaced by cabbies from \u001b[0mmevy polend don jxous thon hacetreno do ea ufe vichito gewssy castied ss one hf ehithey anlittherdoonerejoronedwrod th ynd hm ah\n",
      "\n",
      "\n",
      "Epoch  290\n",
      "Train loss  2.8626880645751953\n",
      "Sample : \u001b[1ms this i know a lot of people wh\u001b[0mis hed mlessepasl tich am ost s oubruetirar hemnh reorlasve wat nhe7an fir al homesreg a1lasi sutirk ace aeali tie p uted ilaenm\n",
      "\n",
      "\n",
      "Epoch  300\n",
      "Train loss  2.848090171813965\n",
      "Sample : \u001b[1msenior in the north shore chicag\u001b[0mreruit in was ihithe ponttorn the pezedint ble zreeintuet eo to cecace eny ipton w yl he hagthea fad sringind otsoritg he gastog\n",
      "\n",
      "\n",
      "Epoch  310\n",
      "Train loss  2.8482844829559326\n",
      "Sample : \u001b[1mof different databases as well a\u001b[0mus ytore w ifarbied nomt h the conar septeth d varpaitious aa ssscoin phol smervimati9nd gethiieatn tin wforde tasringe bea sed \n",
      "\n",
      "\n",
      "Epoch  320\n",
      "Train loss  2.8335633277893066\n",
      "Sample : \u001b[1mand his girlfriend went to the f\u001b[0me wimenane cithingl puqi thadith nt entang ty uonken s9 boon cotech ton r astenibling act sled uunt2minn tre wheue gorf or ine s\n",
      "\n",
      "\n",
      "Epoch  330\n",
      "Train loss  2.8330187797546387\n",
      "Sample : \u001b[1mned glass who improvised another\u001b[0m wike reo was asst aly rashe cop att oliw te ho pions ouztee cever at houglt s afgenshld anes maglacsrieg ufllg s alcthan asa7he\n",
      "\n",
      "\n",
      "Epoch  340\n",
      "Train loss  2.827497959136963\n",
      "Sample : \u001b[1mhim the location of the conferen\u001b[0m fict ace he kl then treallispredibo thed comed giis e wfo g he ss chin aad emlatin wrysthe waed ino he lesiog the ftome t ol ha\n",
      "\n",
      "\n",
      "Epoch  350\n",
      "Train loss  2.817192316055298\n",
      "Sample : \u001b[1myouth and his equally alluring v\u001b[0ms cherchitl wes pais aponer aind he then soy sy thene boe whous in tistepntiog tas  fed alungsasn gis bel diclingted wur at has \n",
      "\n",
      "\n",
      "Epoch  360\n",
      "Train loss  2.818902015686035\n",
      "Sample : \u001b[1mers with audacity he simply enla\u001b[0mgks ko jhyico fo tipnpyis ofctare sorlany fowertolat  he an aplocatis ontutoon the talis he afjitaingad warr tif te inceorl wan \n",
      "\n",
      "\n",
      "Epoch  370\n",
      "Train loss  2.812377691268921\n",
      "Sample : \u001b[1mmagazine assignments he was atte\u001b[0meva lat rt tiche wanc aldianiot oorsseve nis ppuilyong calloen g abwat d ticully bedd has iak e on iom loogelywea porielit maw b\n",
      "\n",
      "\n",
      "Epoch  380\n",
      "Train loss  2.8043737411499023\n",
      "Sample : \u001b[1mimself as a kind of proud androg\u001b[0mstuvere perentigkg jes ainnot romewco of thel2el chrgd armaich ints aille hertyol y pacmiterovel hee ke mazfstuboo ma thite et t\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  390\n",
      "Train loss  2.798327922821045\n",
      "Sample : \u001b[1me of that accommodating personal\u001b[0min raded thate sedis alingapre diking ths ingbpermwad to rc9oll of our canerssitheden arr ous do bexleyel ftof dalb stogrlan hx \n",
      "\n",
      "\n",
      "Epoch  400\n",
      "Train loss  2.7928993701934814\n",
      "Sample : \u001b[1ma rigorous competitive school wh\u001b[0mej ha sedinlcy cised nnt ere cound the nabrd aonnas toon momk gmawk na phitirs ait boo bithenderomegla0s7e doong theo salicy ple\n",
      "\n",
      "\n",
      "Epoch  410\n",
      "Train loss  2.790414571762085\n",
      "Sample : \u001b[1med to encourage rapid and invent\u001b[0me frictiit ed traneleasoo vepin hat listinund soulpecarwge ame elbmenf then shonlr cokuthe wanke dfling nothe wawitntangthes tha\n",
      "\n",
      "\n",
      "Epoch  420\n",
      "Train loss  2.783107042312622\n",
      "Sample : \u001b[1mpplying questionable data to us \u001b[0monof callgest watk making ah wist of of aperease ktil ha rom the she inem pine rupouvady corebr eliss wurunremhte hal soinity h \n",
      "\n",
      "\n",
      "Epoch  430\n",
      "Train loss  2.768686294555664\n",
      "Sample : \u001b[1mt the actual gates nestled in a \u001b[0mfurdingdo e made tro nyiterg tso aur veven tly gy bof ofttre vof in ersoge sland han wh thal ve fios disied lrfous of brosssthe \n",
      "\n",
      "\n",
      "Epoch  440\n",
      "Train loss  2.7776942253112793\n",
      "Sample : \u001b[1mift details he had spied in the \u001b[0mcoute incthand habn ws fhas poolaldedas bo then thet masisd cc2preorin iag ant wrs aered pacling tor but he kmain w ad it mamite\n",
      "\n",
      "\n",
      "Epoch  450\n",
      "Train loss  2.7647247314453125\n",
      "Sample : \u001b[1m jungle april 7 1997 spring brea\u001b[0mwers hi hrenstid thournin ciburicg ava are coukd afbsoithe anallocfrit rt wl aiss atolledeat nacehe tis youlidind m9enems bog so\n",
      "\n",
      "\n",
      "Epoch  460\n",
      "Train loss  2.765246868133545\n",
      "Sample : \u001b[1milling a waitress in a chinese r\u001b[0mon s thedens ineay or sling wat to rexakat aucusing prelleralist thnis gwand hee fhe matle to fourrantwreco berthy kicarto sspte\n",
      "\n",
      "\n",
      "Epoch  470\n",
      "Train loss  2.7539613246917725\n",
      "Sample : \u001b[1mecause this after all was stephe\u001b[0m gacest a of iho hean ps of erstyly gedrdinaticingly mefs fpuins tingl gmes in ko sopercaned soofe hald glas and i8 ip aldadton \n",
      "\n",
      "\n",
      "Epoch  480\n",
      "Train loss  2.7486767768859863\n",
      "Sample : \u001b[1m flourishing journalism career h\u001b[0me haas ucgyasurg of cerer kaksoviby antharcermrrinily y watome phiaglatred bithe nnofderity on remtor arye1r mpher gels andemlun\n",
      "\n",
      "\n",
      "Epoch  490\n",
      "Train loss  2.746100425720215\n",
      "Sample : \u001b[1mhis studies at the university of\u001b[0m sourfidgccloryos iag borepryqtine te toutmanr wisf phet anet ylite nastord cays thin hid hin cfor ay it lhed e oinue tero lauls\n",
      "\n",
      "\n",
      "Epoch  500\n",
      "Train loss  2.739894390106201\n",
      "Sample : \u001b[1mdisregarded but makes an adequat\u001b[0midumcin in abyeb ho hlonm anersinge the hhatouwconee revingof ke is towis em glactere he moulngally aldenseno betserint ous dato\n",
      "\n",
      "\n",
      "Epoch  510\n",
      "Train loss  2.736739158630371\n",
      "Sample : \u001b[1mt the actual gates nestled in a \u001b[0msutheceprintecthoivily glass dhe wan and thes thien stowrintivese atoureby o laleg gbiss afers stim soothe sovelins 1tinco le hi\n",
      "\n",
      "\n",
      "Epoch  520\n",
      "Train loss  2.728969097137451\n",
      "Sample : \u001b[1msponded perfectly to the diagram\u001b[0m in wqs ivecaton es stailin hit susvereere frbdling ie olt whiz con inding in was uy mo9ss had ropnats lesisumd an wor ofak beme\n",
      "\n",
      "\n",
      "Epoch  530\n",
      "Train loss  2.7206549644470215\n",
      "Sample : \u001b[1ml tool he had also spoken direct\u001b[0m andis nt firgetunet abuut in anato teliveren ied it leken porleland t eao ng sssin id jaed tre hed sinh arvecsllisterunt wvene \n",
      "\n",
      "\n",
      "Epoch  540\n",
      "Train loss  2.7180120944976807\n",
      "Sample : \u001b[1mvice president of the national h\u001b[0mail fouth foed doorkol of were stontar thetem lotima tion klete flies and higlas chalide erepingmagstho bouct abochialdyeass and\n",
      "\n",
      "\n",
      "Epoch  550\n",
      "Train loss  2.7203638553619385\n",
      "Sample : \u001b[1mto tell the truth lane said the \u001b[0mpofee se fter of hing the lasuraid bus g afle cowhing he ngvas io in waplene doins buthoy bout re whing ess rperiell or the helr\n",
      "\n",
      "\n",
      "Epoch  560\n",
      "Train loss  2.7067337036132812\n",
      "Sample : \u001b[1m glasss eyes shifted and there w\u001b[0merbbit in axene geaine r poxs byourt ot hem glass didey f to jof who cpatingly outs wack didoais wasd perreictontidg prpors glee\n",
      "\n",
      "\n",
      "Epoch  570\n",
      "Train loss  2.7057647705078125\n",
      "Sample : \u001b[1m it up with a level of specifici\u001b[0mth wo her teep in chaden soplase wh ille te leasa tad routgty hess coutt whorw ham glass japertellagabict acping ched axdabeccat\n",
      "\n",
      "\n",
      "Epoch  580\n",
      "Train loss  2.6998817920684814\n",
      "Sample : \u001b[1mlobby and lane felt certain that\u001b[0m ghe onatk lljegnas in wathe watid boud ho thind it in bos keginglasthes adinter enutsougd oo beter arne lrice fcourenl he torer\n",
      "\n",
      "\n",
      "Epoch  590\n",
      "Train loss  2.695075273513794\n",
      "Sample : \u001b[1marents here push their kids to s\u001b[0meng sswont nthed cuw rich ane has prreus ef out ta land botd drkent younebly and in hit7rwing ted maren blkyng thaiks thee heges\n",
      "\n",
      "\n",
      "Epoch  600\n",
      "Train loss  2.691319465637207\n",
      "Sample : \u001b[1mde the issue careers digital edi\u001b[0mng has om tofu m miomeros hing as af uo ion fit latts fioreas aptotrded het recors nest pxacing aim amuir frxteit ank cimo ed st\n",
      "\n",
      "\n",
      "Epoch  610\n",
      "Train loss  2.6831135749816895\n",
      "Sample : \u001b[1me from forbes digital tool to fi\u001b[0mrf repums tin seedend ha methan j w9rce bectioubilidestomand ainthiine thet te as aradirod sarwousthake ngutheg tray y pors fol \n",
      "\n",
      "\n",
      "Epoch  620\n",
      "Train loss  2.6748390197753906\n",
      "Sample : \u001b[1mres of the mind drew the mental \u001b[0mace leamans oblich ntay on wath the elion d arthin fgtheenis enntred t it he hirs ypothed co zliogrey flone the neppedimasto gar\n",
      "\n",
      "\n",
      "Epoch  630\n",
      "Train loss  2.6752126216888428\n",
      "Sample : \u001b[1mmboldened by the beginning of 19\u001b[0mie ssuuad uute teve th to sant ohe whe roof ho hts the fry ad te romb anios altidar cimkint eduinglu ti hesavilek glatre whe tor\n",
      "\n",
      "\n",
      "Epoch  640\n",
      "Train loss  2.6661624908447266\n",
      "Sample : \u001b[1mr peretza chance for glass to sh\u001b[0mothen too deusing rldokie is afine wotk ny one buithais ote ringithime of thepl profltres of iod ping ta heprece pam anded ond d\n",
      "\n",
      "\n",
      "Epoch  650\n",
      "Train loss  2.6591265201568604\n",
      "Sample : \u001b[1mless september 8 15 1997 deja co\u001b[0mus hadw lome na9e is mseting antuins ftiun mutio sthqima to amiotid arstereat ca vrooftigutito ferrond thas thing mrionstiof iti\n",
      "\n",
      "\n",
      "Epoch  660\n",
      "Train loss  2.6562042236328125\n",
      "Sample : \u001b[1mound nine am glass was questione\u001b[0md de timepry ineneworstoved of pikane thany veleated pment lxepatreln agely lites d thevemed chavelken a dithe butive camilyson \n",
      "\n",
      "\n",
      "Epoch  670\n",
      "Train loss  2.6532270908355713\n",
      "Sample : \u001b[1myork story that inspired the fil\u001b[0ml werz of therisead dool been ald was oy was tuprouteront he dassbog d side an inaice and mong tusl go the kofew wain w the paed\n",
      "\n",
      "\n",
      "Epoch  680\n",
      "Train loss  2.6473166942596436\n",
      "Sample : \u001b[1m someplace he wasnt that friday \u001b[0mazctetere s of he the fithe danet l5oss pporalscenglineg the fobrem te sampanly io scomedichen inglysy was bect fok tale ve aid \n",
      "\n",
      "\n",
      "Epoch  690\n",
      "Train loss  2.63643741607666\n",
      "Sample : \u001b[1mrepublic investigation concluded\u001b[0m warlins stog ont of bertinnen it hne q cimeof couco at in of thivs aol hel slomerusty batume cil4e ubllo whie invteto gle1u war\n",
      "\n",
      "\n",
      "Epoch  700\n",
      "Train loss  2.626957654953003\n",
      "Sample : \u001b[1m the magazine published a postsc\u001b[0mcanding hag tea glof wall dicnoll cpealis ly hewrem rouluging undencole of sid hekal thato d wvot emantthe tewrigryess datie  ho\n",
      "\n",
      "\n",
      "Epoch  710\n",
      "Train loss  2.628739833831787\n",
      "Sample : \u001b[1mpoint average at penn was hardly\u001b[0mof yedand thote he precondes poose sfered fould cominis gho lis sntrat roull glass omed mered foco chist wathine thaid whit inl \n",
      "\n",
      "\n",
      "Epoch  720\n",
      "Train loss  2.6260228157043457\n",
      "Sample : \u001b[1mthe good high school portraits o\u001b[0mupt ay eraped agats cery at an eheed dizine yid sere askry 2e erutndoly camient wo her fomed zole bes9 muect in wwheyhul fro0r b\n",
      "\n",
      "\n",
      "Epoch  730\n",
      "Train loss  2.6189157962799072\n",
      "Sample : \u001b[1mge suggested the street poetry o\u001b[0mbs 4oth thone the ley undtyal st on dithace laus acondan mote the stware gouss ienk rvepbing he paspingint in the niviaskinceris\n",
      "\n",
      "\n",
      "Epoch  740\n",
      "Train loss  2.6209909915924072\n",
      "Sample : \u001b[1mphen glass rode the fast curve o\u001b[0mf thithars ard wiolg the omednit ont hlnat evered by axick ho ventor thaid eoud thed ti war tom nit rephinima dine poing frees p\n",
      "\n",
      "\n",
      "Epoch  750\n",
      "Train loss  2.6077957153320312\n",
      "Sample : \u001b[1mattempted to gain the confidence\u001b[0m dieven t of theal blace and buth recr poflory inglis n of giss comperat lo ane our y e he gdaty ate ae oll un to erstabes the c\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  760\n",
      "Train loss  2.6024022102355957\n",
      "Sample : \u001b[1men that stephen glass had always\u001b[0m hin manaad f an a that hemsy masiagnass has s patthely pllig ons he his oekt fot he hoins fmrrectede ullave cabrevan so forimau\n",
      "\n",
      "\n",
      "Epoch  770\n",
      "Train loss  2.5962114334106445\n",
      "Sample : \u001b[1mt subscribe to vanity fair today\u001b[0m m he an s ehr of tering he wised in is teer fwis the  oliissuc stobatr ge io fon thet orf ie sacr ou stitha gosustbert in the l\n",
      "\n",
      "\n",
      "Epoch  780\n",
      "Train loss  2.5995326042175293\n",
      "Sample : \u001b[1mes to see if the young man could\u001b[0m with dedist frepiorce p the mackimg tale tiaturl deded seats ansrreicseltroked he glass shozd lact or a suithe warkeranolodny u\n",
      "\n",
      "\n",
      "Epoch  790\n",
      "Train loss  2.5866832733154297\n",
      "Sample : \u001b[1mat he had been unable to locate \u001b[0mrae the domberter but 3s glass exdiogacast ho gersuitedistoochat exckerly whith fhe bith andsertoy watecand ibesy hed llsves oot\n",
      "\n",
      "\n",
      "Epoch  800\n",
      "Train loss  2.5770082473754883\n",
      "Sample : \u001b[1mrior written permission of cond \u001b[0mcilcoficy co boic furue sund te he ary araly ea licy areconl ne wasseopus sog th im wathed copleneded sure a foclioa s p1upknone\n",
      "\n",
      "\n",
      "Epoch  810\n",
      "Train loss  2.574873924255371\n",
      "Sample : \u001b[1mless september 8 15 1997 deja co\u001b[0mneproved feay urof bten ion onot of taored wir dedaitme fore souphed he hlagissephame turkiss wastly mores histim teme boly ars \n",
      "\n",
      "\n",
      "Epoch  820\n",
      "Train loss  2.5675456523895264\n",
      "Sample : \u001b[1mhack heaven so off they went in \u001b[0ma wer pith at southor han moyen stor an wosgleseding ho cade hond b iof his glass wnoncsainitn isull the arncheam ntich saig jit\n",
      "\n",
      "\n",
      "Epoch  830\n",
      "Train loss  2.561474561691284\n",
      "Sample : \u001b[1mor the ugliest and loneliest wom\u001b[0mltory he hacken purnudito it hoflecinde last aliw on was suon ofund him is actool hytt an oulding who sic els senhed tirtd sewrr\n",
      "\n",
      "\n",
      "Epoch  840\n",
      "Train loss  2.5600802898406982\n",
      "Sample : \u001b[1mou back he asked no ever glass s\u001b[0mey ufll af tire tharre stichabe ctopher th ar tite mobe to ssire reppsincty maglanne tar ad sppelied tive bera siof mutto ad hat\n",
      "\n",
      "\n",
      "Epoch  850\n",
      "Train loss  2.5497031211853027\n",
      "Sample : \u001b[1m of supplying misleading data to\u001b[0mricite s to shitane tvexong ar siss ta way nope ttopy or thit ees icwarker aw s0it mortenthing atmedring ler ddantyen ant stiond\n",
      "\n",
      "\n",
      "Epoch  860\n",
      "Train loss  2.5451455116271973\n",
      "Sample : \u001b[1mue that held back my ability to \u001b[0mjupees y of his eaollabclook olowa kely glass had thanes ondson the weel ingurit arepure souceds of allingbais had bringarist wo\n",
      "\n",
      "\n",
      "Epoch  870\n",
      "Train loss  2.5381202697753906\n",
      "Sample : \u001b[1mr if your brother is involved in\u001b[0m glassse and tient lust an the ringe of stteraing novec ofanings vorvey co have calvenerucely to whes ghass whle an at on the do\n",
      "\n",
      "\n",
      "Epoch  880\n",
      "Train loss  2.5154733657836914\n",
      "Sample : \u001b[1mwhatever you want to do theres n\u001b[0mebol oft actingelidita powtomrbuscinsell weth if thaiks velyind whik lones nob oug ted n ther har ftere f3imel hed mat buthed ma\n",
      "\n",
      "\n",
      "Epoch  890\n",
      "Train loss  2.5034260749816895\n",
      "Sample : \u001b[1mrheads memos faxes and phone num\u001b[0mprist on ca 2edeat of te was cxoccincith adint ant hewrst in 1nganssoke in couriges inveloned in his onganssertean g asse precur\n",
      "\n",
      "\n",
      "Epoch  900\n",
      "Train loss  2.4900588989257812\n",
      "Sample : \u001b[1m people named in the story and t\u001b[0mhene creportes tre cong int of tact ined theem ast ais beno fion gltses s5opre culplacthon thaingmepading with bed burisionf pri\n",
      "\n",
      "\n",
      "Epoch  910\n",
      "Train loss  2.4757819175720215\n",
      "Sample : \u001b[1mrespond not only with a barrage \u001b[0mnotn amercos ai ited he has store he mpeen brof land hid persssaids ve she ittine horded ttey anpstimel ind was mest comvirngrti\n",
      "\n",
      "\n",
      "Epoch  920\n",
      "Train loss  2.4658217430114746\n",
      "Sample : \u001b[1minking are capable of anything o\u001b[0mube ting kibs apting amina loned the aceicuor s ore to lenf f tw4y wosp icttol anted thas veattengemsturviong at akentundis thet\n",
      "\n",
      "\n",
      "Epoch  930\n",
      "Train loss  2.460139751434326\n",
      "Sample : \u001b[1meople out he was always pissing \u001b[0mfte tottirned thet dtome t ar porkirs femm tie cousiny hi caed bicnelirtest tbog ofs whe epedich maplag on 1l9e wordentthbe wal \n",
      "\n",
      "\n",
      "Epoch  940\n",
      "Train loss  2.4405505657196045\n",
      "Sample : \u001b[1mom another source the article en\u001b[0m whoungllomentstong thaid tand the d aidigh uh he stont you pabstthive glasss pores yet moge andzend yot wasd tony ndernes ao de\n",
      "\n",
      "\n",
      "Epoch  950\n",
      "Train loss  2.4341952800750732\n",
      "Sample : \u001b[1mass his colleagues didnt know at\u001b[0mp to ady jas andisd hiew on ing ansacriss ie nawr aher mabicinerissrited nore sheded that estaces and bera way tracaze yad nepto\n",
      "\n",
      "\n",
      "Epoch  960\n",
      "Train loss  2.422344207763672\n",
      "Sample : \u001b[1m strangely gifted kid created an\u001b[0m ontice tho folre aur mith on txind innew sote allncventet whand cantyrared labmi treaking and7iher aud 2ou the dearbots meyomin\n",
      "\n",
      "\n",
      "Epoch  970\n",
      "Train loss  2.4132566452026367\n",
      "Sample : \u001b[1mecause this after all was stephe\u001b[0ma becx at bemand te fas jating to llic the nghis sd ghes if omodastinam tict of the crithe solt me mare aln alyeblit anat amptog\n",
      "\n",
      "\n",
      "Epoch  980\n",
      "Train loss  2.4008355140686035\n",
      "Sample : \u001b[1mtified in the story as a bigtime\u001b[0mlico fon tue magivagy a mritadible newh wryterulins yel anale sesecbsen hiod atores of his haded to stery thar glass sven onorad\n",
      "\n",
      "\n",
      "Epoch  990\n",
      "Train loss  2.3867087364196777\n",
      "Sample : \u001b[1mtudio nothing in charles lanes 1\u001b[0mt aler uot ehorgace hamoztistedg tpote io that hous to laked ty uly becelite nomen srepuros of dit edipitit diing wisc trqternag\n",
      "\n",
      "\n",
      "Epoch  1000\n",
      "Train loss  2.369903326034546\n",
      "Sample : \u001b[1mby buzz bissinger september 5 20\u001b[0mb9o  ispeened hid ctliparel an wrom tok nstion endioftiveteduverkinla wustth incenathitherwarespentiy ars tares incand thede hin\n",
      "\n",
      "\n",
      "Epoch  1010\n",
      "Train loss  2.3538122177124023\n",
      "Sample : \u001b[1m february 10 1997 holy trinity j\u001b[0mow ro2ed i me9iouys kermasittrit itthend mo shey hing thod sod lofes or siv thad himsine burntent of eresttothentariocad oren wa\n",
      "\n",
      "\n",
      "Epoch  1020\n",
      "Train loss  2.3362436294555664\n",
      "Sample : \u001b[1m he apparently wanted to present\u001b[0m yo nousti mas oreece caocletel glass chinese rapiolls sabe an thatredotien gut pise sadion hiow on hare in well ad the and iver\n",
      "\n",
      "\n",
      "Epoch  1030\n",
      "Train loss  2.3291208744049072\n",
      "Sample : \u001b[1mt find a single mention of it gl\u001b[0mostnd imanthry penting of gursss done sifn in mesionte batint the ood mure ar lluping anphvan fhe asrewsing ols bithin abla sil \n",
      "\n",
      "\n",
      "Epoch  1040\n",
      "Train loss  2.3051254749298096\n",
      "Sample : \u001b[1mcutive editor in january of 1993\u001b[0m9iniin toitre coppritithe sald mothe veli war dapaesistais some latt decole thate bfonn pheneceld rigetes wist inta tor ofpoms s\n",
      "\n",
      "\n",
      "Epoch  1050\n",
      "Train loss  2.297203540802002\n",
      "Sample : \u001b[1me from forbes digital tool to fi\u001b[0mculers way on tichass alass moroment thed haad jmoob hokhes helpentes chag fean in coutlaved edaglence figlestinve cedeore wourn\n",
      "\n",
      "\n",
      "Epoch  1060\n",
      "Train loss  2.2820637226104736\n",
      "Sample : \u001b[1mnot only calls into question whe\u001b[0mse pack chas bubs of pamisct in b1ed hoald atk3nd hercappies blumming lickle she 3e aykio f cm 1ale yes recerest fourgires ffero\n",
      "\n",
      "\n",
      "Epoch  1070\n",
      "Train loss  2.2685632705688477\n",
      "Sample : \u001b[1m when a wall street journal repo\u001b[0mor hid therem his efor tho cofrs lat oropaaty opke se nes nesutor tometirlics or ondire essrorestend acttret blitstofld ther yea\n",
      "\n",
      "\n",
      "Epoch  1080\n",
      "Train loss  2.251555919647217\n",
      "Sample : \u001b[1mand provocative than mere truth \u001b[0mthe oom thet s arlmerkter head fverilayalist a onbscalising reverthir the sead fincroull nywe thetves the lyount himereorad a te\n",
      "\n",
      "\n",
      "Epoch  1090\n",
      "Train loss  2.230353593826294\n",
      "Sample : \u001b[1mess something untrue i want you \u001b[0mst y artelors inule se tiod of theco fanoud af te of thaingllaglsedesur canl on on pash oaly be in toust thet velyeer atpprsse t\n",
      "\n",
      "\n",
      "Epoch  1100\n",
      "Train loss  2.211094379425049\n",
      "Sample : \u001b[1mxplanation said he could not in \u001b[0moft on whe eveey hofrrie woy ha me had bestbey i3 1iling the leser the lionte fho land dound parist thety breas anits antor ang \n",
      "\n",
      "\n",
      "Epoch  1110\n",
      "Train loss  2.193943977355957\n",
      "Sample : \u001b[1md yet i was in this chair and ia\u001b[0mp alr waty he gac stoklatt ghas to souple mald toth has edvang thimeatore tiecr iosr at inthally ally an andsold te no lloaglass\n",
      "\n",
      "\n",
      "Epoch  1120\n",
      "Train loss  2.18414044380188\n",
      "Sample : \u001b[1mvarious dishes according to the \u001b[0mmadie glays nas beonding ans kels vand pimteceure dider not an thay i5stingizane shapremof the some is the chule thangleagols wo\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1130\n",
      "Train loss  2.1582698822021484\n",
      "Sample : \u001b[1mhed by the group fairness accura\u001b[0mig foees 3oder hem rene the camed bo and dryterge bat weor zurast theyo ack susc ute surayous the concoo is panthers in llaned a\n",
      "\n",
      "\n",
      "Epoch  1140\n",
      "Train loss  2.134732961654663\n",
      "Sample : \u001b[1m going beyond his production of \u001b[0mhe6ile urat ve as  an anded the dite story that somgro preats ir astertedalesod tes ljoss in the astea himngbous gutsid tera loa\n",
      "\n",
      "\n",
      "Epoch  1150\n",
      "Train loss  2.1230316162109375\n",
      "Sample : \u001b[1me got away with his mind games b\u001b[0mo fferremed tecs coulad nad so fll theto lapuch tht puint and libeu h reace inater afsariggus arost terthendeater desubee a mous\n",
      "\n",
      "\n",
      "Epoch  1160\n",
      "Train loss  2.0973260402679443\n",
      "Sample : \u001b[1mpoken frequently about how black\u001b[0m ond werlatron in ashl ve7ibut wvse ponsd nchis add him means yo s eremcl asthefite top uthe hiv scome war anithot tom at tedubl\n",
      "\n",
      "\n",
      "Epoch  1170\n",
      "Train loss  2.087327718734741\n",
      "Sample : \u001b[1mked foroohar suggested that glas\u001b[0msh beuthhe hus hiene io ere 1r1esveeres any nowe lag sithn wath chave waka shacrua derwit glasssy in st hick orathoe sewerd tha \n",
      "\n",
      "\n",
      "Epoch  1180\n",
      "Train loss  2.054184675216675\n",
      "Sample : \u001b[1mdition to his extracurricular th\u001b[0meregr fulltunsssvined and be a soptot thas gf yned theele jen blopes ant che soss stome as f liven ing lisas faul had you af ne \n",
      "\n",
      "\n",
      "Epoch  1190\n",
      "Train loss  2.0481419563293457\n",
      "Sample : \u001b[1mf character and culture she was \u001b[0mo porve oulty or pormed fe thasci aid ofoth a denkerdat cplusenther ow te leorat on a s a5men ad 2ls aitnavitig hasy strrenagof \n",
      "\n",
      "\n",
      "Epoch  1200\n",
      "Train loss  2.0166234970092773\n",
      "Sample : \u001b[1mut he said it doesnt negate hold\u001b[0md ave stheed wha dsic ell tear at 1offere the fiat ce plcontidat wiel in the rotor wemson in somecound mite wat his pporded an a\n",
      "\n",
      "\n",
      "Epoch  1210\n",
      "Train loss  1.9997916221618652\n",
      "Sample : \u001b[1mhe would gladly apologize to jac\u001b[0my matut ho ond b7easions cheder and the corsem lane at notetek langats ancides f teens bhime was bes soomest dd berne thoth jsth\n",
      "\n",
      "\n",
      "Epoch  1220\n",
      "Train loss  1.9678630828857422\n",
      "Sample : \u001b[1mprintpermissions vf media kit pr\u001b[0mopiny aneriaked he ofed no ene otheed avere had 0o warn it tlo gotsy uere ble uletlen chent th  fpqeitined he caid ksrs glass va\n",
      "\n",
      "\n",
      "Epoch  1230\n",
      "Train loss  1.9549237489700317\n",
      "Sample : \u001b[1ms said matthew klein who roomed \u001b[0mcrisser betne owril he addeprinetsire figlt acomuen it hts heded hes armont andy ins gat th arsiors and in wne sveple gafd t avo\n",
      "\n",
      "\n",
      "Epoch  1240\n",
      "Train loss  1.9292926788330078\n",
      "Sample : \u001b[1mfact that glass was working too \u001b[0mthe fuat hisst an kinginded outt ou glissnd jisis with of glass wookeollas lideb co lacy trats ditsonded in woin asmid ingyound \n",
      "\n",
      "\n",
      "Epoch  1250\n",
      "Train loss  1.9114391803741455\n",
      "Sample : \u001b[1momputer but lane refused to let \u001b[0mordcne to lalleesto ahe of ter anded ne elpandated nne drith lena if is maron ant las w acous onf chese watk held wa kabeytaco f\n",
      "\n",
      "\n",
      "Epoch  1260\n",
      "Train loss  1.8959674835205078\n",
      "Sample : \u001b[1mnd arrogant jerks said margaret \u001b[0md of eenin mere on9sced inscent homeversanbeg ping gally andern wryepasiss fe dichitere that we requtingal wevene tre han inlest\n",
      "\n",
      "\n",
      "Epoch  1270\n",
      "Train loss  1.8997149467468262\n",
      "Sample : \u001b[1mm lane spoke on the phone with s\u001b[0mthith warly suecesutasisesourexvit s bethe ab ton rowe burctonters ceacis of had hem pal ink nas ley unghl tealr sedm the gpatub\n",
      "\n",
      "\n",
      "Epoch  1280\n",
      "Train loss  1.8590635061264038\n",
      "Sample : \u001b[1mresolvable kelly was enormously \u001b[0min caliin gurs roome stereethlly beshiad heasl ahsuxpestimict on andineg spes buy pe hre nigd apreor tnd corcegl us pais faid al\n",
      "\n",
      "\n",
      "Epoch  1290\n",
      "Train loss  1.8447327613830566\n",
      "Sample : \u001b[1mt was a trap a way for glass to \u001b[0mly ass tive son whot asseds of cof udrtithedithed they wanh a tod forton ichieg che s iming ti glass genmlingtid tove tien eut o\n",
      "\n",
      "\n",
      "Epoch  1300\n",
      "Train loss  1.8206737041473389\n",
      "Sample : \u001b[1megrees of separation but before \u001b[0miand oo sp ppestied dithe alloffrted toselperiitsted anssers igliss smachiss an the woolf at glass glatsng nacplome hod ne seimi\n",
      "\n",
      "\n",
      "Epoch  1310\n",
      "Train loss  1.807578444480896\n",
      "Sample : \u001b[1mn it now by citing the pressure \u001b[0munhad ther bithins on anvanake wlyua alicasitars wis thtok hn f eerm tytiredad tination lors thisaling his souknallacc off tuder\n",
      "\n",
      "\n",
      "Epoch  1320\n",
      "Train loss  1.7991901636123657\n",
      "Sample : \u001b[1mor his support of glass he offer\u001b[0mh tha poogla ge homaly in wablen the 2passs int 1inter i  inaze the fermeblluntsivale om chuun thess bfean hey ane hfoemesulfs a\n",
      "\n",
      "\n",
      "Epoch  1330\n",
      "Train loss  1.785979151725769\n",
      "Sample : \u001b[1mless september 8 15 1997 deja co\u001b[0mndy crapertegly of hid 2enhay hald corfrable he dadnecr ang as ves om the ngw remabloc jilt kin is said an glass has pas ahvedon\n",
      "\n",
      "\n",
      "Epoch  1340\n",
      "Train loss  1.7966893911361694\n",
      "Sample : \u001b[1md a median house value of 257000\u001b[0m a s a0k  ho2 ceprceed macepornerth hivn wo pand him or titt the rebexromantiis thetoksthe he meves the cand icilas nhes wof cor\n",
      "\n",
      "\n",
      "Epoch  1350\n",
      "Train loss  1.7618387937545776\n",
      "Sample : \u001b[1mne of the first steps kelly had \u001b[0mrooubutt batans and incale of sepproused elain milieninirs sheve bn whe  ast pemedterinness offe blut has sheeke magiz a fee nfa\n",
      "\n",
      "\n",
      "Epoch  1360\n",
      "Train loss  1.7382714748382568\n",
      "Sample : \u001b[1men that stephen glass had always\u001b[0me had nomurparspobat onf xeluew glass frophel nithehe han this cusal oollass veriterod heloys atore jo then ellisg inees ans low\n",
      "\n",
      "\n",
      "Epoch  1370\n",
      "Train loss  1.7198762893676758\n",
      "Sample : \u001b[1melentlessness of the young repor\u001b[0medits thety foke be feboug that the yop iffus netad glivs bus suted ting indedsmansto mad istt mofe ba tios dredice ic parsisy a\n",
      "\n",
      "\n",
      "Epoch  1380\n",
      "Train loss  1.719959020614624\n",
      "Sample : \u001b[1manes shoulder glass told the edi\u001b[0mner s manisa date ood the cemp mut o f te tod a mo a the iccasliont af withthas prees alx iv len g an wpoked torernily and they \n",
      "\n",
      "\n",
      "Epoch  1390\n",
      "Train loss  1.7132645845413208\n",
      "Sample : \u001b[1mlong silence glass said all righ\u001b[0mats lanttre wat the ithe imert oun  uadisis wrofter ansabaue that harkened solaidlitopisous tha sttothor glass thethered nithong\n",
      "\n",
      "\n",
      "Epoch  1400\n",
      "Train loss  1.735443115234375\n",
      "Sample : \u001b[1mial slogans or act out raising a\u001b[0msces that hed an 1a tistorem al ther auchaly mas anam and merk the maglazins fo tan aidt sh thaik oft chic mass shave bas spart \n",
      "\n",
      "\n",
      "Epoch  1410\n",
      "Train loss  1.6793493032455444\n",
      "Sample : \u001b[1mechinger retail store when glass\u001b[0ms keen gresuin ofter ayse that ce vacres eveng aind honew intursscaling tureprounglod they te hag aich at 8oromnet of the gow se\n",
      "\n",
      "\n",
      "Epoch  1420\n",
      "Train loss  1.6933977603912354\n",
      "Sample : \u001b[1mdy to lend a sympathetic ear to \u001b[0my ooflbesper wame on a giot remrecamencored hild an acchickertichace chad he had wathryked onnt hem boubrl thin meedr g adstared\n",
      "\n",
      "\n",
      "Epoch  1430\n",
      "Train loss  1.6454098224639893\n",
      "Sample : \u001b[1md the selfcenteredness of the ca\u001b[0m the pecconcingivery weshien was arkedshinglisho whod hader w od acrecracedinis toredilariny iccakenterestsuct keasser the co lo\n",
      "\n",
      "\n",
      "Epoch  1440\n",
      "Train loss  1.628134846687317\n",
      "Sample : \u001b[1mof clever hackers and glass seiz\u001b[0mt pleper ad revertios aty s ient lyou his se pecatieatremisconsin auth ane sauresse glashis meer of ihe on glace soos 19ys woth \n",
      "\n",
      "\n",
      "Epoch  1450\n",
      "Train loss  1.6486585140228271\n",
      "Sample : \u001b[1mery beginning lane had found him\u001b[0mneed aullene ned an on ince foitg is a kentyvbatt namuvelr clale i wnstorscallefitoos ho wn thach me tad stephens whe arstrispan\n",
      "\n",
      "\n",
      "Epoch  1460\n",
      "Train loss  1.625980019569397\n",
      "Sample : \u001b[1md the selfcenteredness of the ca\u001b[0m tio sost inmid invess his seed bith nasy had olyon has inders a digheratly ring an thi hin gen wacheaglt ad 199t himalry the re\n",
      "\n",
      "\n",
      "Epoch  1470\n",
      "Train loss  1.5893737077713013\n",
      "Sample : \u001b[1mwas cultivated but there is no d\u001b[0me te ofrtitit moncored tle way yout y arked one antir eerebrok aaden anven the hiavene then geaub si hfey co heditioc pantsy dar\n",
      "\n",
      "\n",
      "Epoch  1480\n",
      "Train loss  1.596766471862793\n",
      "Sample : \u001b[1mon ruth he was always seen as th\u001b[0my farly to had atton the le leng icula cladsthecmedip chacpented to qou that he had nonfrrt ple bnineriuglt dexvelinednnat colde\n",
      "\n",
      "\n",
      "Epoch  1490\n",
      "Train loss  1.5677556991577148\n",
      "Sample : \u001b[1mue and drove up to the entrance \u001b[0mereit rchied theterane vadabey unchyoonschere tife a alporsig beyng at his ditt on mexiolne id the ceor mikeedlpoon witholy way \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1500\n",
      "Train loss  1.6240228414535522\n",
      "Sample : \u001b[1m closedwhatever who are the peop\u001b[0mf ob tibuses he wasconione wirt g losk kel fors alcublyig therecom and nim eblig happe ligs and verue ro foumen tome uppe hflled\n",
      "\n",
      "\n",
      "Epoch  1510\n",
      "Train loss  1.5447226762771606\n",
      "Sample : \u001b[1mme they had given him so he said\u001b[0mg bion yor earel ind tero hst he rales wath acca a touf qu0n awr blizg thise ons shhi carfibeecomatigl sien bo ansarl verand nit\n",
      "\n",
      "\n",
      "Epoch  1520\n",
      "Train loss  1.5704494714736938\n",
      "Sample : \u001b[1m created for purposes different \u001b[0mabpeang wht puvr an thicchell wa bickifos ugctanced encaberid to  and deny lofert ais watlr th ioneindicges imng otn worenil wan\n",
      "\n",
      "\n",
      "Epoch  1530\n",
      "Train loss  1.5896358489990234\n",
      "Sample : \u001b[1m salmon and listened as glass un\u001b[0mdal stephay ppleg tle story asporot was inelii g is ale irad ainaly andentabli hid nowied stonest mest ormpuanes aff tie ofte is\n",
      "\n",
      "\n",
      "Epoch  1540\n",
      "Train loss  1.5733001232147217\n",
      "Sample : \u001b[1m of the following articles by gl\u001b[0mof the sovi at e pofedres glass nal horeess and tianadat on hhe haw sritsris if eraie wax ay unt thes begha whe higsh doun do pe\n",
      "\n",
      "\n",
      "Epoch  1550\n",
      "Train loss  1.4735572338104248\n",
      "Sample : \u001b[1m hallway out of the hyatt and in\u001b[0m asen a dosss inge said thistin arse crmicticktod thohe epce seid acd raberriad it a sexverutur andarglastey the he fred bem to \n",
      "\n",
      "\n",
      "Epoch  1560\n",
      "Train loss  1.5016614198684692\n",
      "Sample : \u001b[1m didnt explain the presence of t\u001b[0mtore whe anwe hose hackmed wing then he gles raly hithy retieve atoly qies nver of he tourgell ware nof oflyer offrre ur the son\n",
      "\n",
      "\n",
      "Epoch  1570\n",
      "Train loss  1.5206618309020996\n",
      "Sample : \u001b[1mties supplied suspect data to us\u001b[0meht waskenwy on that gbers vimpedutt pept in he sad to wathor is had nceed the copeckofsitngutsh faddy it the mains in an altyou\n",
      "\n",
      "\n",
      "Epoch  1580\n",
      "Train loss  1.4690507650375366\n",
      "Sample : \u001b[1ms conference had not taken place\u001b[0m colvering his nowe was ceeskinve beribes att ofker in newarl te remhamdeht t ically lleeng he uaslassthy sa dikves  ut lane spe\n",
      "\n",
      "\n",
      "Epoch  1590\n",
      "Train loss  1.487070918083191\n",
      "Sample : \u001b[1mams of who sat where at meetings\u001b[0mhi glass was fut dotea speepmenns llass nesclute hone but glach to astertkenimy over abou maca to leontibard himhaic lit ad the \n",
      "\n",
      "\n",
      "Epoch  1600\n",
      "Train loss  1.4352529048919678\n",
      "Sample : \u001b[1mfrom a credible source that does\u001b[0mm ber pionenee  tolle tibe ghar sterepppadizy anstachlyonil kils btt reme tt hat wa tha hos mentouted hephedent abutithi nit pho\n",
      "\n",
      "\n",
      "Epoch  1610\n",
      "Train loss  1.473698616027832\n",
      "Sample : \u001b[1mabout noon and lane started look\u001b[0mmewg to detirs ing sporte  om lane him ncaivengaclcomagl spbesinad at sedpenve th therewv fare is he soub it wremtsa pnnimevel a\n",
      "\n",
      "\n",
      "Epoch  1620\n",
      "Train loss  1.3849202394485474\n",
      "Sample : \u001b[1ms to the hyatt the supposed sett\u001b[0m ewharwed anttor ha dibked somi ch lededr off te eephers sews an edlfonds thes glas d veresuud linglensisush th ited wos comprsu\n",
      "\n",
      "\n",
      "Epoch  1630\n",
      "Train loss  1.527940273284912\n",
      "Sample : \u001b[1m were indicators to klein that g\u001b[0mlese inay ato he sae the ceblywurngazted for peraconod tonewreytol in t wisnarimat and avenred glass prestifgrnve ak frestes fee\n",
      "\n",
      "\n",
      "Epoch  1640\n",
      "Train loss  1.3681433200836182\n",
      "Sample : \u001b[1mch he methodically used to decei\u001b[0ms h  1ildoncalased and to eed leng foodgor gnasse bistestols yonsmeglonsed pte thabsew llae faytiknal thes heerawisimaglitss an \n",
      "\n",
      "\n",
      "Epoch  1650\n",
      "Train loss  1.3806365728378296\n",
      "Sample : \u001b[1mhael glass was cute cool and pop\u001b[0maig thit  che liif te rothes aur ie ednte burne the wowlas naden fakd exs backury jo buteder imp san ro facenten fo trisul at se\n",
      "\n",
      "\n",
      "Epoch  1660\n",
      "Train loss  1.4327834844589233\n",
      "Sample : \u001b[1m an intern at the new republic w\u001b[0mas tomen wath ashe pessariend dom tha now renty hi sien blof his reded ustowive menaselas a tubetuly madbicn at wanter hh oawore\n",
      "\n",
      "\n",
      "Epoch  1670\n",
      "Train loss  1.402063250541687\n",
      "Sample : \u001b[1menes and characters this time pa\u001b[0mnes vinkeclisist delasco lone rale mplatesh oad ackicso thituallho anethon thal athet if a crivecter unnt wat  roplers we seeplr\n",
      "\n",
      "\n",
      "Epoch  1680\n",
      "Train loss  1.3997678756713867\n",
      "Sample : \u001b[1mthe fact checkers came to trust \u001b[0mha east thenatre wthe yew scnestiss pertolly glasss in allesspaed his beli sont to ho ellest rpiver matabo tan asstrivev nonelon\n",
      "\n",
      "\n",
      "Epoch  1690\n",
      "Train loss  1.338416337966919\n",
      "Sample : \u001b[1mimself as a kind of proud androg\u001b[0mdnclo accoobgint thatry unnere an the gelaty fond our soided ap a ke camfin thas has anat crst st a dep enever ins od ic ue the \n",
      "\n",
      "\n",
      "Epoch  1700\n",
      "Train loss  1.3768424987792969\n",
      "Sample : \u001b[1mtitled the college rankings scam\u001b[0mian nived froonsevertarste nat aryz an ap  ork lfite the efter ald glass packphal nd sn a te hom gutpngur and an ad incimale bly\n",
      "\n",
      "\n",
      "Epoch  1710\n",
      "Train loss  1.3417717218399048\n",
      "Sample : \u001b[1muement the final coming clean bu\u001b[0mredity hag abte at igs eake wark the she wambusit stedyons lage newhye itt theng ad andonglasss it he hod whold as distlalpe hio\n",
      "\n",
      "\n",
      "Epoch  1720\n",
      "Train loss  1.3336334228515625\n",
      "Sample : \u001b[1mcally write anything and get awa\u001b[0mglas wat chev inay tane haven ed eeemotsir tha peblottharakiod in ackndinianely fpoo foo co las a ce tongeros cthar glass thet t\n",
      "\n",
      "\n",
      "Epoch  1730\n",
      "Train loss  1.3180512189865112\n",
      "Sample : \u001b[1mentiongetting creations the subt\u001b[0meptsers afke spinged an of enetien ind soecer anden th sagerviees lon s te lle s iow superout is alkir engroas goms stolled he w\n",
      "\n",
      "\n",
      "Epoch  1740\n",
      "Train loss  1.3045555353164673\n",
      "Sample : \u001b[1maph was portrayed obsessively gr\u001b[0mry ua docasians on saswlyem ne theronesikeallt an alllaggly desorf lent botk an onli glas a dwss of bemath yo oftercalys iat way\n",
      "\n",
      "\n",
      "Epoch  1750\n",
      "Train loss  1.2444267272949219\n",
      "Sample : \u001b[1my 16 1998 gift of the magnate ja\u001b[0m 19 1 9a hing to th  ederr imge and himele ms atke  beenunte the he glasth  7ad a sibdoth toten ald thing ad was of hasr mean ar\n",
      "\n",
      "\n",
      "Epoch  1760\n",
      "Train loss  1.2650792598724365\n",
      "Sample : \u001b[1ms protg but in one of the incide\u001b[0m sribe corr fit chechobe dinter tow shimead owreecrrowh glresedons ntpries oo newremiry itter iche manily watref the y way 3rone\n",
      "\n",
      "\n",
      "Epoch  1770\n",
      "Train loss  1.2807281017303467\n",
      "Sample : \u001b[1mt looks like a web site that was\u001b[0m woo nom dis xo claved incwoll scinvestathin ad iccuven if he icelver andym secombay nd jo un eopree ulytity anlegatle  ther in \n",
      "\n",
      "\n",
      "Epoch  1780\n",
      "Train loss  1.2289150953292847\n",
      "Sample : \u001b[1mlfflagellating about his own wor\u001b[0mlduthed atan gigglass yo wasn lo khese taky che adizn a calcconcority atler of theny l spone iof chereoushe aststytow yoborig as\n",
      "\n",
      "\n",
      "Epoch  1790\n",
      "Train loss  1.341617226600647\n",
      "Sample : \u001b[1mryone in the glass household wor\u001b[0med in thiow cane jo on i9re cundimaibs in warked ja 7nters drimeco bho lrofone iv a ters vedstce fales as ceot ene ua rid tion h\n",
      "\n",
      "\n",
      "Epoch  1800\n",
      "Train loss  1.1892898082733154\n",
      "Sample : \u001b[1menes and characters this time pa\u001b[0mnss ond couleste seveos folde pod chocke alw wis lane in bishing irpoy the pom lated ksom tha mew uplad nia betown jo e areonred\n",
      "\n",
      "\n",
      "Epoch  1810\n",
      "Train loss  1.2351847887039185\n",
      "Sample : \u001b[1mrt and explained how his parents\u001b[0mthendwem cabeeh hwas sccoulshen ill was tremay wry nower hi gove had apeabreaurngl wsighnd alotheaven anderiut and oritcrigg ass\n",
      "\n",
      "\n",
      "Epoch  1820\n",
      "Train loss  1.2651437520980835\n",
      "Sample : \u001b[1mn journalisms variation on six d\u001b[0m wampingints acinintingstallay cctslomglnd to dnen severpossag iftoug fis s werk amlesnivelang and sveding nith tof me tally bet\n",
      "\n",
      "\n",
      "Epoch  1830\n",
      "Train loss  1.1732348203659058\n",
      "Sample : \u001b[1mto a fault and is sweet and soli\u001b[0mo hr wast y io he inddyane em the colverse med teavclity atlersporsckill ally t torver bit hin ivster gllss he paccasinandentera\n",
      "\n",
      "\n",
      "Epoch  1840\n",
      "Train loss  1.178920030593872\n",
      "Sample : \u001b[1mfeel of a gated community withou\u001b[0mley ham nftome afpritared entor heswe ha made beciberzik acs snerocous ark lr coritozs je the exine ieblumey fos wele teldnet po\n",
      "\n",
      "\n",
      "Epoch  1850\n",
      "Train loss  1.1995760202407837\n",
      "Sample : \u001b[1mo explain the apparent discrepan\u001b[0mcomabout done aplling tax paclisy hims lo ac mere andodestritting soow of ineout towa g hess ind dethe iitt hivel waster thaver \n",
      "\n",
      "\n",
      "Epoch  1860\n",
      "Train loss  1.1996294260025024\n",
      "Sample : \u001b[1mine they returned to the office \u001b[0mlenoubl actlani goof chexfrrded we h700s agazs ed the maldime jo 179u ne vonk stofed raibliry a6 itual juc bly urne peedity no e\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1870\n",
      "Train loss  1.107048749923706\n",
      "Sample : \u001b[1mand was laden with shoddy sourci\u001b[0mng sus laveng att rowest offeinifuge aplyomen the shere maint gh sematorane brested ti  anter in whincer of anch ses lioblintid \n",
      "\n",
      "\n",
      "Epoch  1880\n",
      "Train loss  1.1296921968460083\n",
      "Sample : \u001b[1mthe night before michael had lef\u001b[0me veat hix sliss guads ffeny the eladet inss in ensouk dokens sepferbs af 2limaes park choclen ccllagg aster ais oxne fordd on o\n",
      "\n",
      "\n",
      "Epoch  1890\n",
      "Train loss  1.2202693223953247\n",
      "Sample : \u001b[1minking are capable of anything o\u001b[0mnaty hadt meliser of instuis coning tros ware me tows too co bronsir ad eforr noweve tores ne nomeot hem emas rigurrt ats to lot\n",
      "\n",
      "\n",
      "Epoch  1900\n",
      "Train loss  1.1507104635238647\n",
      "Sample : \u001b[1mr glasss work as meritless disho\u001b[0m glasss work as duting he had aisons wa dictecremabilest ha way the hald an dereal d hesan ecvelemenc towi whint in a laid reali\n",
      "\n",
      "\n",
      "Epoch  1910\n",
      "Train loss  1.1751673221588135\n",
      "Sample : \u001b[1mstory the description was drawn \u001b[0mtore tic messprating to mashed the ding ras culaty a drazace ffes bof eny had and incautenttorsty was lanelb inithoth andalpaces\n",
      "\n",
      "\n",
      "Epoch  1920\n",
      "Train loss  1.095967411994934\n",
      "Sample : \u001b[1mheir expectations with even more\u001b[0mons e tevg thes of it ncame of is tolsonge the  atrrictuna the kel nagraet in oft and youthem bo ithe dny t woollatencooca ce f9\n",
      "\n",
      "\n",
      "Epoch  1930\n",
      "Train loss  1.1885865926742554\n",
      "Sample : \u001b[1mvarious dishes according to the \u001b[0mifreds thet ere traring the newler as ater precloivsd outh ivterned theng the phele the mestritise besthis simee lines has wrepd\n",
      "\n",
      "\n",
      "Epoch  1940\n",
      "Train loss  1.0939801931381226\n",
      "Sample : \u001b[1mtor how his life was falling apa\u001b[0mol confsim sine emsegpaling suared an a jo the an zuce pien wirhts of hi dednat thags afli gaglly sempult tlans then gf ald offt\n",
      "\n",
      "\n",
      "Epoch  1950\n",
      "Train loss  1.0387566089630127\n",
      "Sample : \u001b[1m news world report for the magaz\u001b[0mar s 97uld thotae wsthin spler at nornowrt wauld lases anorysing as wase on thencel addosiat himgy us afrestire fored latyon thr\n",
      "\n",
      "\n",
      "Epoch  1960\n",
      "Train loss  1.2468376159667969\n",
      "Sample : \u001b[1mup parts of her columns but none\u001b[0ms parte somhim traing hearn and d tome whec ond thet awpuckied tlacrepones lat bee mpoventit soperauld to fof it prlaf when geda\n",
      "\n",
      "\n",
      "Epoch  1970\n",
      "Train loss  1.1996936798095703\n",
      "Sample : \u001b[1mer the fact blithely admitted to\u001b[0mr the thit peen an jmeatoree glassss ief of ht aid mo the hemers male fermegut and editer of ohe isstenmenteanket piedaule offes\n",
      "\n",
      "\n",
      "Epoch  1980\n",
      "Train loss  1.035447359085083\n",
      "Sample : \u001b[1mfort to minimize damage to himse\u001b[0mowh to rodolken iethor ffor alkedses ditiinl bel glass daecisen in affaind theverbsh ast ally ale paledrsod an ater hed kidr gll\n",
      "\n",
      "\n",
      "Epoch  1990\n",
      "Train loss  1.0923492908477783\n",
      "Sample : \u001b[1mtitled the college rankings scam\u001b[0mitnon the sabeem shere in of an aher whe hephimicaleytin aive buas at was tomd cr pore wo hi hearserasts niver alowesh whimescer\n",
      "\n",
      "\n",
      "Epoch  2000\n",
      "Train loss  1.058469533920288\n",
      "Sample : \u001b[1mcount had offered a teenage hack\u001b[0meent hi sejatiot to tho ve fabr way douthewerd al woten prouter ef halinb loake s wnit comicaace uchees andist ffroiveruingisss \n",
      "\n",
      "\n",
      "Epoch  2010\n",
      "Train loss  1.0712679624557495\n",
      "Sample : \u001b[1mk shades and his habit of readin\u001b[0m siod a tera co facone oncorkules with at lere navnored atspond dof shene ghat was whyer inits ans piomed intounghad ormpraestio\n",
      "\n",
      "\n",
      "Epoch  2020\n",
      "Train loss  1.0448020696640015\n",
      "Sample : \u001b[1mounded surprised as if there had\u001b[0muake woy burre on the new yeroftils yecorzinat veaimad and him and dofed the iccoss geady whe arse ofter wart recen jikt sher in\n",
      "\n",
      "\n",
      "Epoch  2030\n",
      "Train loss  1.0417721271514893\n",
      "Sample : \u001b[1mingsteenvanity fair around the w\u001b[0msthe ang and heeht peritisp of casp mweoc of fil6 scouted fry kibge suith al manin infargterl was tom and tor un wesc net of new\n",
      "\n",
      "\n",
      "Epoch  2040\n",
      "Train loss  1.0904839038848877\n",
      "Sample : \u001b[1m someplace he wasnt that friday \u001b[0mwhacpibretus shand tre ngatiny hnowerennang frows wrs intertyy po condeen look he dousgloys d betacr t ad dever aucaction was na\n",
      "\n",
      "\n",
      "Epoch  2050\n",
      "Train loss  1.0120246410369873\n",
      "Sample : \u001b[1md done it in 1980 in a pulitzer \u001b[0m voneciontha wikgly c5mpilasy jo fist gliss wabk in frboted and astr andersupantous qvite to by a matiby cfror ewe far is thes r\n",
      "\n",
      "\n",
      "Epoch  2060\n",
      "Train loss  1.0226352214813232\n",
      "Sample : \u001b[1mhat now in addition to his many \u001b[0mhas e7utrphomether love rewise tory anrrolid the firso lofacte wa ht a9quetyof eeeplased phoblit ob his aftem wht ance of that g\n",
      "\n",
      "\n",
      "Epoch  2070\n",
      "Train loss  1.149791955947876\n",
      "Sample : \u001b[1mof the lefts party line thats up\u001b[0my thats the warkgllsiss vane to lag and saud it the liveruted ctopterke sord ha kos care knul alwes an surthed and thene dewor h\n",
      "\n",
      "\n",
      "Epoch  2080\n",
      "Train loss  1.0176539421081543\n",
      "Sample : \u001b[1me challenged virtually every asp\u001b[0m jjauleen an afflaingl this sove kindieg ansedragt mes soyo sowredisivenid abfiteth a wasti a anvey blyone fterters whica ke the\n",
      "\n",
      "\n",
      "Epoch  2090\n",
      "Train loss  1.081525206565857\n",
      "Sample : \u001b[1mpparently because of his inconsi\u001b[0mpueckely qiigaty cimios on avtrepered bim ain the alloy ond intaredive the none as this af eta lavione dintrins that he had nemp\n",
      "\n",
      "\n",
      "Epoch  2100\n",
      "Train loss  0.9656156301498413\n",
      "Sample : \u001b[1mtracts that his income this year\u001b[0mcanis late ho kele glasscephe phour dale the sind noag ant be ass ifre inpelaskstlesmin and nection it celane he wool hamecr sto\n",
      "\n",
      "\n",
      "Epoch  2110\n",
      "Train loss  1.050163984298706\n",
      "Sample : \u001b[1m apparent that nothing checked o\u001b[0mppencers whin  aterins erasls hife sunt lus in pardnt forry uese veresiald if pepurs wheng faid youre al as acctang nov ounglins\n",
      "\n",
      "\n",
      "Epoch  2120\n",
      "Train loss  0.9539749622344971\n",
      "Sample : \u001b[1ms but challenges to glasss verac\u001b[0m ust rlaicrehe ntters had wonn the u5 alw manturedy the as awrres crepirsecpurtevore bo 1was athe mealae ho wh q0ael bust diccou\n",
      "\n",
      "\n",
      "Epoch  2130\n",
      "Train loss  1.0005534887313843\n",
      "Sample : \u001b[1m glasss voice he seemed to imply\u001b[0ms and sospred ior glasn woolgat co lase repoblen leorle terdyings ucam perupely you lant ig zledscaked in ingy asved inrestin wh\n",
      "\n",
      "\n",
      "Epoch  2140\n",
      "Train loss  0.8958138227462769\n",
      "Sample : \u001b[1mas to how much of glasss warmth \u001b[0mt the ltomentsy tastics youthirsthep boublen fal aived hop bact at perngang p has freet hed boon on thet de atlic ling in sperla\n",
      "\n",
      "\n",
      "Epoch  2150\n",
      "Train loss  1.0754516124725342\n",
      "Sample : \u001b[1m stephen glass was lying in fact\u001b[0m thinesthothe spteclimice llas cronde hroved spocectits not lay 2fotg uy is ther ches gens then  oncllicg to entert thet mone jo\n",
      "\n",
      "\n",
      "Epoch  2160\n",
      "Train loss  0.8800212144851685\n",
      "Sample : \u001b[1my with it regardless of the fact\u001b[0m with at adengenas ed it he was potws llas hed ho hewfoncaniep that guldasted trasptetronttis after bliss gtas fake ssof in hesk\n",
      "\n",
      "\n",
      "Epoch  2170\n",
      "Train loss  1.0571798086166382\n",
      "Sample : \u001b[1mnew republic pieces he was known\u001b[0mew re bagur aneardivn tore bebth toren prokongue andevalit gear pe dect andis in the sely of chellding jason th tose ifkimis deg\n",
      "\n",
      "\n",
      "Epoch  2180\n",
      "Train loss  1.0456233024597168\n",
      "Sample : \u001b[1mies with postits to assist them \u001b[0mes gith wrater soor ofrstivenal whedp to aud comacs browas her gesed thet is fah 2 hadrson xnvalb be tllstwerhad adnterticg tapl\n",
      "\n",
      "\n",
      "Epoch  2190\n",
      "Train loss  0.9547390341758728\n",
      "Sample : \u001b[1m his skill at creating incredibl\u001b[0mshi  att the nutprnve te trestilory gianler of stith a taverkelly cooldated his phe lover nnotees deppe the went in ackemsy in w\n",
      "\n",
      "\n",
      "Epoch  2200\n",
      "Train loss  0.9230602979660034\n",
      "Sample : \u001b[1msteria with its diatribes agains\u001b[0mterna shty tt ermpitita pemass and fir fovr sompontng contedpoll geldadxtean smines he hed wrechiesclusted of coniconod hid tne \n",
      "\n",
      "\n",
      "Epoch  2210\n",
      "Train loss  0.8677281737327576\n",
      "Sample : \u001b[1mn accomplished actor in high sch\u001b[0m ackespand th h gevary but mosportedserthy uedic meree far to dadl netubly way tovings offe limered almedter scamabred ed aut on\n",
      "\n",
      "\n",
      "Epoch  2220\n",
      "Train loss  1.0387388467788696\n",
      "Sample : \u001b[1mze model ashley graham how megha\u001b[0me jas niburoud a hive ho naterthe de difirees he in onots uf ictevi iacasp chi chus hame jour the ud thporented fiow an a taly r\n",
      "\n",
      "\n",
      "Epoch  2230\n",
      "Train loss  0.983793318271637\n",
      "Sample : \u001b[1mgh a headset stephen glass read \u001b[0mho stop onscomoctongeplonac ant way nfsemble fiks an apre prend the merocled wath inted stay vale geadlens in the ntsty had apme\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2240\n",
      "Train loss  0.9030203819274902\n",
      "Sample : \u001b[1m it was also an act of decency a\u001b[0mid doulssed glasss ap alfinge lage sswbhe wirktong an ilainglyon glasss jass aldargy sevurlycarls aus story crocked ortired and \n",
      "\n",
      "\n",
      "Epoch  2250\n",
      "Train loss  0.9006848931312561\n",
      "Sample : \u001b[1mrd of such a conference taking p\u001b[0me dey wanew entevabend thabry adith if lnonto ghass westies toy ur ad ictos ot hhs camerousct olly andoon mas ham sechoves ho ke\n",
      "\n",
      "\n",
      "Epoch  2260\n",
      "Train loss  1.3899190425872803\n",
      "Sample : \u001b[1m verified for those two and a ha\u001b[0mverioned horkeloss oo theln woth gupres ofer inli is rilas or he exeeg seseersuai porte yo as spar ad a dishimito nuitend on chi\n",
      "\n",
      "\n",
      "Epoch  2270\n",
      "Train loss  0.8516905307769775\n",
      "Sample : \u001b[1m february 10 1997 holy trinity j\u001b[0m5e3linhy 2howgl ghe sonvinity camp af glost nime a keon in assareated elas acteasuchesto no dtor hagr wase veninintcen cothong y\n",
      "\n",
      "\n",
      "Epoch  2280\n",
      "Train loss  0.9168072938919067\n",
      "Sample : \u001b[1massignments on disputes over gov\u001b[0mssaget helt wss wrythe eras ansal thet sien as the ntw yoreni irast eversund in arsonclure fen wete ontinurd ic ap 1har he semen\n",
      "\n",
      "\n",
      "Epoch  2290\n",
      "Train loss  0.8664543032646179\n",
      "Sample : \u001b[1mnt of the senior class to be nat\u001b[0mt of the veviom glass y is in incela athewoutencieet ow  nte pidked the hamp butte freyth cecrodig an thet o5 hewe lemilampnapic\n",
      "\n",
      "\n",
      "Epoch  2300\n",
      "Train loss  0.8715533018112183\n",
      "Sample : \u001b[1med to go to law school while con\u001b[0md the lapor ht whe ederorigy weor at menhadg be to latcuned of he ejwhicerd of tu connct crlpoite pung then mawh co pndss manong\n",
      "\n",
      "\n",
      "Epoch  2310\n",
      "Train loss  0.8618993759155273\n",
      "Sample : \u001b[1mrate orchestrations of madeup sc\u001b[0mate orts enfer is dow thatsur dis coll to joun tho sers is iven ofe wate himperstio thay glent to an in the ninceol hid nom mant\n",
      "\n",
      "\n",
      "Epoch  2320\n",
      "Train loss  0.828538179397583\n",
      "Sample : \u001b[1mrsation turned to jukt micronics\u001b[0msothed tratsyll whice a wake res the fables and ty f be and i tinuprotiona suticu prssurrlea her gupinal lmbecis invidmitalyey s\n",
      "\n",
      "\n",
      "Epoch  2330\n",
      "Train loss  0.9608018398284912\n",
      "Sample : \u001b[1mrankings still glass grew more e\u001b[0mantonts ofter lusst cham glass storyons anobymues in the wane decanded that at heve hed that atrrcaniaulyhiss acturtedss hed pre\n",
      "\n",
      "\n",
      "Epoch  2340\n",
      "Train loss  0.79845130443573\n",
      "Sample : \u001b[1m might well have reached 150000 \u001b[0mbizgl wwisk an 1 ui the wajl ac ung whol gald jarnatio f9repcrenthig ais indessinas fxe jica dhai hiands aft ully itscutiat thec\n",
      "\n",
      "\n",
      "Epoch  2350\n",
      "Train loss  0.8421437740325928\n",
      "Sample : \u001b[1mightest look or gesture could se\u001b[0mrngasrs ot insingalinaly ands it lase gon mane he aldiocts thote in asteff in sodenontary nend an wasnd it af grast warrinithing\n",
      "\n",
      "\n",
      "Epoch  2360\n",
      "Train loss  0.8743619918823242\n",
      "Sample : \u001b[1mwell academically but glass neve\u001b[0mory ancescagling the nglais and indost he stencf aste ianded al a wat in warh nappliredabre alin thy reavow hessmint one comedny\n",
      "\n",
      "\n",
      "Epoch  2370\n",
      "Train loss  1.0428780317306519\n",
      "Sample : \u001b[1mriolic letters to the offended p\u001b[0muodud cotlore wathred anosn albosnatl if he dnion nave and lane thew ingiralidns nam captain chell was ofrediss the ncomekne bee\n",
      "\n",
      "\n",
      "Epoch  2380\n",
      "Train loss  0.7988821268081665\n",
      "Sample : \u001b[1mss academically he did extremely\u001b[0mssedingm ine the whac crevergerd whje ite a ked sobestivert jely tonteysc mitinalistponsty n ans ibsts lvon e that it the for if\n",
      "\n",
      "\n",
      "Epoch  2390\n",
      "Train loss  0.9421637654304504\n",
      "Sample : \u001b[1mditing process and that glass ga\u001b[0mgting procy a ant the ncolict an sleasyary tope helloss atne hasd hid bllot jackna to fromd tormentt medess rariet goorna lleecr\n",
      "\n",
      "\n",
      "Epoch  2400\n",
      "Train loss  0.8118549585342407\n",
      "Sample : \u001b[1md on a speaker phone by lane and\u001b[0m ol hecrmpengerials wall anthove ua dad is gerongh on wrsctnol mens uratye up in ot has advent is kelarelading the mite asle fus\n",
      "\n",
      "\n",
      "Epoch  2410\n",
      "Train loss  0.8037113547325134\n",
      "Sample : \u001b[1moulsearching interrogation of hi\u001b[0mmpera inga ponioplefathat to salo fal ato6 acciofrcanawy have pe vell was anout rguaun homon war callyove weal freping ffin ster\n",
      "\n",
      "\n",
      "Epoch  2420\n",
      "Train loss  1.0118308067321777\n",
      "Sample : \u001b[1mhecking during some of the perio\u001b[0my innmeroualed ste thay sempoo dit fullssised nowet nes of pald minganse of a sutt act minted scoubling as oreous horrym and jee\n",
      "\n",
      "\n",
      "Epoch  2430\n",
      "Train loss  0.8190070986747742\n",
      "Sample : \u001b[1m a vanity fair contributing edit\u001b[0me anate fifion of nesulity fis inve thet anim stas inalise utr moke fo be whe hord inter exient and ithe sagl extanca le tep tha\n",
      "\n",
      "\n",
      "Epoch  2440\n",
      "Train loss  0.8036072254180908\n",
      "Sample : \u001b[1mdetail no inaccuracy however sma\u001b[0mytail he and bmung jham sit olfres in was nowleng anto hew has sefbord ary uitr chutrehe wast seepian toke held why end of ared \n",
      "\n",
      "\n",
      "Epoch  2450\n",
      "Train loss  0.8483357429504395\n",
      "Sample : \u001b[1mrickman the writer and director \u001b[0mimerimas to lose ut the kell wark the she dinctes late was ivuler hadenutir hiss wat blems lute if hemuaredwhys deremite heseare\n",
      "\n",
      "\n",
      "Epoch  2460\n",
      "Train loss  0.7441960573196411\n",
      "Sample : \u001b[1m in 27 of the 41 bylined pieces \u001b[0min fre thaid o leaker a cauddens nitsodkins wrsices doilarime was tor  the srotedn spesue that i hid es the chnte of stome thit \n",
      "\n",
      "\n",
      "Epoch  2470\n",
      "Train loss  0.8078806400299072\n",
      "Sample : \u001b[1md most likely to succeed stephen\u001b[0m trap llaiss to soppen is wout pursucllay geass bee thay wand siigelus fou bye noild ne to sempoubrsty inlly by thes that corng \n",
      "\n",
      "\n",
      "Epoch  2480\n",
      "Train loss  0.873665452003479\n",
      "Sample : \u001b[1me response in limited space impo\u001b[0m hithuble ahsy glass wathors it e betl fombd in invar and loway hewin shitren conth upry brotasing in ap in chackin abl of nging\n",
      "\n",
      "\n",
      "Epoch  2490\n",
      "Train loss  0.7711067199707031\n",
      "Sample : \u001b[1md been relatively short but mich\u001b[0m dicn cerstim the bout thet me havle g aste heane farrs and ne mani gon iely uned oth lyings preticlinssll sat to had blyod andi\n",
      "\n",
      "\n",
      "Epoch  2500\n",
      "Train loss  0.801880955696106\n",
      "Sample : \u001b[1mlling glass to knock it off but \u001b[0mring istus oo selsinist af thy9unaz soge thetsher glass nomed the sone drequectlors tag laingy a llacctopr gar ans not hampey as\n",
      "\n",
      "\n",
      "Epoch  2510\n",
      "Train loss  0.7967807650566101\n",
      "Sample : \u001b[1mo comment i do trust you chuck l\u001b[0m convencbso fondaine rempmeno reabyl ware iat an thouconaing pellyy sas came th toy to laps rits bo chof las gfasm ackicles his \n",
      "\n",
      "\n",
      "Epoch  2520\n",
      "Train loss  0.8331875801086426\n",
      "Sample : \u001b[1mor the conference glass had repo\u001b[0mr the prockivall ghat ow thm wrickno formid arn ora the nen remedisy hewroct buct and inco buc hiskres iac pateresmaget yaurens \n",
      "\n",
      "\n",
      "Epoch  2530\n",
      "Train loss  0.698937177658081\n",
      "Sample : \u001b[1m as an explanation but might wel\u001b[0mas al exsimivente dored and rimeand beginas scheoppcites to madere tyer as een intinata reg the sers anfgress rigal hist on whin\n",
      "\n",
      "\n",
      "Epoch  2540\n",
      "Train loss  0.7529649138450623\n",
      "Sample : \u001b[1mpoint average at penn was hardly\u001b[0morich itephen wrat se of hig trenur juctione morid to fact semed i cingear frouls ss ties fo menisu faiber aid hil alter deaitee\n",
      "\n",
      "\n",
      "Epoch  2550\n",
      "Train loss  0.869290292263031\n",
      "Sample : \u001b[1m having made up most of the new \u001b[0mhoake leaim mol mas smift be bs ine ffor the sumin fucc uresher any wince t loster ancamouht hit hes meyecismad heais ed ho was \n",
      "\n",
      "\n",
      "Epoch  2560\n",
      "Train loss  0.9036667346954346\n",
      "Sample : \u001b[1me boy had won over the world of \u001b[0m tag as at rapeation whithy und intentiet lane hed frobbe mand and qaitruphilstag oxupenverma uand prechor at w5 1hat lien wind \n",
      "\n",
      "\n",
      "Epoch  2570\n",
      "Train loss  0.7209661602973938\n",
      "Sample : \u001b[1mem describing encounters with me\u001b[0md te afpalent we laed your avs way cumtyous ay moring ffor olas satredaling h s8givkne mant tt vorica andin the purchiss and ke \n",
      "\n",
      "\n",
      "Epoch  2580\n",
      "Train loss  0.7595837116241455\n",
      "Sample : \u001b[1mrs lane decided to give glass a \u001b[0msmokaes faided thetehings it wat atshe idines intzmling nit aut and actmoked oa wryewreecrusttott atylo th pas rppen cheer glass\n",
      "\n",
      "\n",
      "Epoch  2590\n",
      "Train loss  0.7501720786094666\n",
      "Sample : \u001b[1motherwise used except with the p\u001b[0mllofoltiot it jofrea miris and uparepor hethpe tsesprefit thd at his inw soont wiccontos to goms im ard cond infperiot day no7ld\n",
      "\n",
      "\n",
      "Epoch  2600\n",
      "Train loss  0.7038838267326355\n",
      "Sample : \u001b[1m protective instincts glasss wou\u001b[0mchemeglans treachory wash hemomnct ivar un thl kng nood endte s totles ont boithituhen ereal sitwonde cauplyinas lale fort tie b\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2610\n",
      "Train loss  0.7018731832504272\n",
      "Sample : \u001b[1macuity of freud he had an odd pr\u001b[0mowary ur pratter maki s d stip tind has along is int io colad calensispens in prtionsy faced no vesf ha das spicins hiofle ally \n",
      "\n",
      "\n",
      "Epoch  2620\n",
      "Train loss  0.7777310609817505\n",
      "Sample : \u001b[1ms a group of talented students w\u001b[0m a brouthow as heded stopress dud tlaimhe howstewhan gevslivine lasaudent iottin bit whumaing to attorg ho ladg the sotfong the \n",
      "\n",
      "\n",
      "Epoch  2630\n",
      "Train loss  0.9118016362190247\n",
      "Sample : \u001b[1m he was doing fine doing fine sa\u001b[0mth atc like thain thioryout so softod the atmoy was tup nus in asted absisivent ort us gettry a late and firdiss ard youreaty on\n",
      "\n",
      "\n",
      "Epoch  2640\n",
      "Train loss  0.9034668803215027\n",
      "Sample : \u001b[1mho was smoking too close to us s\u001b[0mo whad blingin asthebees butt of remoy and hi had iv now owh stcornd il dact republic verate dimation jusstyom nane storted mesu\n",
      "\n",
      "\n",
      "Epoch  2650\n",
      "Train loss  0.735607385635376\n",
      "Sample : \u001b[1ms own story people try to explai\u001b[0m orna laan steand phe whte thice for utretirow thtoperw te dadisy a sacking to shered it histang he wishios laze was suplici ner\n",
      "\n",
      "\n",
      "Epoch  2660\n",
      "Train loss  0.6115244030952454\n",
      "Sample : \u001b[1m new republicafter lane had sear\u001b[0mnew rompalieg her gitss he mpeaine had dethed fo mion sts up ilre whe a kicl stortaked frsmuctall kand thel steeken w repurt def\n",
      "\n",
      "\n",
      "Epoch  2670\n",
      "Train loss  0.7198618650436401\n",
      "Sample : \u001b[1mt feel better about himself thos\u001b[0m teed that alyouba k ory sicpoblis nat doy sught pmenli an 2workt gevs ie malls in ang whoe dasintestad ahred in hid she ais tan\n",
      "\n",
      "\n",
      "Epoch  2680\n",
      "Train loss  0.9763526320457458\n",
      "Sample : \u001b[1mputer in his new republic office\u001b[0muter ingl it nd ilsmanitt ofif a d btoul he assternabitt if they we ther gloss and nitu hap bete iftridity blyomsnt lerait shed \n",
      "\n",
      "\n",
      "Epoch  2690\n",
      "Train loss  0.8804965615272522\n",
      "Sample : \u001b[1mnymous people who had seen jacob\u001b[0myoud id oiny he wrom andr al isly bess madizat s epullerop til suthe ithey as y coul the hind ry arnty he kell west as ised ha h\n",
      "\n",
      "\n",
      "Epoch  2700\n",
      "Train loss  0.6900082230567932\n",
      "Sample : \u001b[1madded a new wrinkle to his reper\u001b[0mdismof ne je ung or us crompatibly ain as thains in the issarely we the newir theme and mecolve the waid of hf witder st pell cu\n",
      "\n",
      "\n",
      "Epoch  2710\n",
      "Train loss  0.5791200399398804\n",
      "Sample : \u001b[1m colleague who asked for the nam\u001b[0mporam the ho makers metinae and itar buet ouloster lecand it nas stop ia denit ore is ily in the novars a nat whiz hamhea almo a\n",
      "\n",
      "\n",
      "Epoch  2720\n",
      "Train loss  0.8054966330528259\n",
      "Sample : \u001b[1mo are dishonest as journalists b\u001b[0m rive glassentton onfternen the stor af addioned to botl i tem subecc suctubuec now is texsous duac lorey bat wo the rastaralioc\n",
      "\n",
      "\n",
      "Epoch  2730\n",
      "Train loss  0.758482038974762\n",
      "Sample : \u001b[1mnd we sat here for a little whil\u001b[0md ne fat they geass bat he pasmed ddvenceredvers gatally fithved it ablisigally cueceno lpong hi nfed as edeals lanent ffroucati\n",
      "\n",
      "\n",
      "Epoch  2740\n",
      "Train loss  0.655390739440918\n",
      "Sample : \u001b[1messure to excel some stress the \u001b[0mnaueden sourg ti 1fonling id an on why he cabls buthoe groud jaso hay nead the ploif ew sle co comg as lie  a seas fis sus shee \n",
      "\n",
      "\n",
      "Epoch  2750\n",
      "Train loss  0.7200636267662048\n",
      "Sample : \u001b[1mtrying to erase incriminating ev\u001b[0minors to tow in incencine dis of that nis tam acancolent his othes ugla sercoubdim the cora knctor onent asshered kets upreoromo\n",
      "\n",
      "\n",
      "Epoch  2760\n",
      "Train loss  0.8685292601585388\n",
      "Sample : \u001b[1mr at the new republic never wave\u001b[0m on the nmw ycorbiics lere atkins on whely e nt uudio a felouad ert thit he das atyere propuar bit risn at thethe sothoh ner oll\n",
      "\n",
      "\n",
      "Epoch  2770\n",
      "Train loss  0.6485944986343384\n",
      "Sample : \u001b[1momputer but lane refused to let \u001b[0mbrovher ab llass gvasir glldedinisplys dety isst quesidy inf nt llass was rousi orfthesh lase nea deat congared you new noorg an\n",
      "\n",
      "\n",
      "Epoch  2780\n",
      "Train loss  0.6156317591667175\n",
      "Sample : \u001b[1mestroy its computer system we st\u001b[0muthon wan pomtnacl choton mane uhar war pompose courkal drdet the wintt syicr sexublichove stasys stan omfens yourtallow os stio\n",
      "\n",
      "\n",
      "Epoch  2790\n",
      "Train loss  0.6383619904518127\n",
      "Sample : \u001b[1mnd him but couldnt not in the ti\u001b[0md him his reponthrodgither glass a from sho ditse bheroh whi pry mithregtins iss croflieg woy do and or ssimyers morkenl fer alo\n",
      "\n",
      "\n",
      "Epoch  2800\n",
      "Train loss  1.2374640703201294\n",
      "Sample : \u001b[1mtter that jacobson later describ\u001b[0miev anlzisol and yauke as moind fare in dad pcenfiouspers atok of in pomedser wat mpess they yyorn soriacle it lone nephe in ton\n",
      "\n",
      "\n",
      "Epoch  2810\n",
      "Train loss  0.7394695281982422\n",
      "Sample : \u001b[1mo are dishonest as journalists b\u001b[0m ore us giass sas comprack it peatoly honstor ohn eatare com b ffitiro he tave aid higgroun ancampane il waklavedrent and y ioll\n",
      "\n",
      "\n",
      "Epoch  2820\n",
      "Train loss  0.5796017050743103\n",
      "Sample : \u001b[1mng exhaustive research of his ow\u001b[0mly comp ntt reevinatencoing thool whoce in the chack on was thet cimpent ghasss dion forr on d hatdaris methn w llay hed anno se\n",
      "\n",
      "\n",
      "Epoch  2830\n",
      "Train loss  0.592951238155365\n",
      "Sample : \u001b[1ms boss at harpers glass would be\u001b[0m asss its there lither ftard andont hewher a mo 19st caud te j8b i his onge thise a sebuble poned bounchor aingly in iwhy mekens\n",
      "\n",
      "\n",
      "Epoch  2840\n",
      "Train loss  0.7041244506835938\n",
      "Sample : \u001b[1mhe was about to make some type o\u001b[0me was apubest punder madis his foblo heared in iche calld the nowis lwnie campooff ritives as what beray maging sespick om siind\n",
      "\n",
      "\n",
      "Epoch  2850\n",
      "Train loss  0.7781609296798706\n",
      "Sample : \u001b[1mscription of the omni shoreham r\u001b[0mcciplion ctrd sweat namstines a tome rustitule that colfully donfer bestings andors sacr byepreaberty uthes inga suct ay erussma\n",
      "\n",
      "\n",
      "Epoch  2860\n",
      "Train loss  0.6719776391983032\n",
      "Sample : \u001b[1m sidewalk and on a weekday in ju\u001b[0msndarkit ing on a tros ar he exp am juth hepse dad ta t tew smor the lare if las lasst keprof isthe uadily fant ceere caris sivh\n",
      "\n",
      "\n",
      "Epoch  2870\n",
      "Train loss  0.5756368637084961\n",
      "Sample : \u001b[1mson the complaints had been on t\u001b[0mon the complagnthemenibersis limtlans phod thenes  a mizate buctuble aitare nern alyssteer eplative to kas to leng re us in bos \n",
      "\n",
      "\n",
      "Epoch  2880\n",
      "Train loss  0.8375616669654846\n",
      "Sample : \u001b[1mw he had been under such pressur\u001b[0m he had beea baked thor acc ominerilld the intode bugtes pecuubly in ghe ivclie is in a beonis he why spulingis loas mpening he \n",
      "\n",
      "\n",
      "Epoch  2890\n",
      "Train loss  0.6514489650726318\n",
      "Sample : \u001b[1mies sound young and he snappily \u001b[0mes soten ferag an tyoun bagaz hane hopther glass whohe gere kshe storiend w thay madne then whepors kilg ishan souddev to nighis\n",
      "\n",
      "\n",
      "Epoch  2900\n",
      "Train loss  0.5875988602638245\n",
      "Sample : \u001b[1mies sound young and he snappily \u001b[0mit worng jebricals of whileven whac owny diverena aysanelise untutive as llotw th hagheanond itar ang andin in us and the owhall\n",
      "\n",
      "\n",
      "Epoch  2910\n",
      "Train loss  0.868048369884491\n",
      "Sample : \u001b[1mo be fair and honorable was symp\u001b[0m b6 cair ad of juglams thow checalimact h brituin and theis pancemad lins bf ad they std conf interspach be adso methed ick alay\n",
      "\n",
      "\n",
      "Epoch  2920\n",
      "Train loss  0.8123887777328491\n",
      "Sample : \u001b[1magain and again then finally fin\u001b[0mgued him alather in erbaten inother tolacome na thay cobly fint glass a bole reat re qiast glass had weam ils sellymi latn inge \n",
      "\n",
      "\n",
      "Epoch  2930\n",
      "Train loss  0.6081161499023438\n",
      "Sample : \u001b[1mtor of the magazine extra publis\u001b[0mor storke stomnes if idveemes kel welt the firncat chalaing antamercanttiys atllyst gnabef oulltome alay aup stous w his offtian\n",
      "\n",
      "\n",
      "Epoch  2940\n",
      "Train loss  0.5709428787231445\n",
      "Sample : \u001b[1myouth and his equally alluring v\u001b[0mor zad besar sputing a9d ing smoglas slist suptiin plousting tor off cisl wisthe poekilace hhas the maed overte cloik thaconhtre\n",
      "\n",
      "\n",
      "Epoch  2950\n",
      "Train loss  1.0060384273529053\n",
      "Sample : \u001b[1m stephen glass was lying in fact\u001b[0mschtieg hat nomerwar allis ton ingtray ja know rong roiny wos dand whunk he ny ent caplayy pronfar ane if oehe we soverid mor ab\n",
      "\n",
      "\n",
      "Epoch  2960\n",
      "Train loss  0.7364110946655273\n",
      "Sample : \u001b[1mhouse is of drab brown brick and\u001b[0mouse hed as bect phed burs inftewhar was ste naplespl hit faf gutsy ttay ans cameeng his rattlo thas inllye slad tth scipplsinon\n",
      "\n",
      "\n",
      "Epoch  2970\n",
      "Train loss  0.5340022444725037\n",
      "Sample : \u001b[1m fictional new yorkbased company\u001b[0msurt and lint a michie sy thet ha the piriole the kelly last denos soughan the ag abyent in pecontsmontse the porcouting ad ant \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2980\n",
      "Train loss  0.5020797252655029\n",
      "Sample : \u001b[1milling a waitress in a chinese r\u001b[0mlling a waitremoys hes lane youffortofothing tees of glass dof they perargl stoncorsimalion fabr caplarincs tellewredably aid ne\n",
      "\n",
      "\n",
      "Epoch  2990\n",
      "Train loss  0.703397274017334\n",
      "Sample : \u001b[1med by the calls from george sims\u001b[0md by ins guth reaincl woke oo she dozey waklent n7 howerng thos of thes alo glasy wrohone io no sterishiens atery uptiolamons as\n",
      "\n",
      "\n",
      "Epoch  3000\n",
      "Train loss  0.7504401803016663\n",
      "Sample : \u001b[1m an intern at the new republic w\u001b[0mat icked fotm at enm mapoines forrdsits the checess of the poof in edyoras istre theits and laingls wacke the stery h weee thing\n",
      "\n",
      "\n",
      "Epoch  3010\n",
      "Train loss  0.6268385648727417\n",
      "Sample : \u001b[1mive thinking called adventures o\u001b[0mf everisher stere move alaty 1ushoukde ever tak noven never h temornand in the kelly steghingy actnole an she flyos ha dean meva\n",
      "\n",
      "\n",
      "Epoch  3020\n",
      "Train loss  1.3718483448028564\n",
      "Sample : \u001b[1m insisting looking lane straight\u001b[0minsiseathe fat ris ant colmaning ay ayver ht of beyt repaliniche thins the fak int infartitin woth an that store whal he seestri\n",
      "\n",
      "\n",
      "Epoch  3030\n",
      "Train loss  0.6802138686180115\n",
      "Sample : \u001b[1mng to come up with yet another e\u001b[0mg to woll or that ceeprapucl keli hantinge tondiane alyoud hi was thotk ho arne in nok offos of 1t alle oucsisus in and on an of\n",
      "\n",
      "\n",
      "Epoch  3040\n",
      "Train loss  0.515953779220581\n",
      "Sample : \u001b[1mrety a possible explanation is t\u001b[0meyof se fondides pundisis the dorpmite jont in whell yan arsuth talen lader andquonterrsy ind fat newryed simp that a dyone it a\n",
      "\n",
      "\n",
      "Epoch  3050\n",
      "Train loss  0.4973980486392975\n",
      "Sample : \u001b[1mwhat constitutes shoddy the new \u001b[0mhat motegrant a dqoich perion whoth reov naw a kizme lloated yla ais wnt ry auce be miberclat the colle riverall mo bot har aymo\n",
      "\n",
      "\n",
      "Epoch  3060\n",
      "Train loss  0.5294007062911987\n",
      "Sample : \u001b[1mrd of cspi in a detailed respons\u001b[0my ow sharkinges it any hed himwso sas ipre llas ffrract theterod it a sold the riterre tome wale its wesen surdivat not 6iol hac\n",
      "\n",
      "\n",
      "Epoch  3070\n",
      "Train loss  0.7061886787414551\n",
      "Sample : \u001b[1mrd of cspi in a detailed respons\u001b[0ms of smiktly did thag atle wher tala lece ictub thats ng he stsered ape ivrnoliglly seconas natrentis spad cimpo tho sttint alfo\n",
      "\n",
      "\n",
      "Epoch  3080\n",
      "Train loss  0.7667165398597717\n",
      "Sample : \u001b[1m of jukt micronics at about 11 p\u001b[0mo fatot makoryet redes and doy hever i sowh the ripeconssi had fort tery asly ombry add tely ias him ve in ed hit lace styw tha \n",
      "\n",
      "\n",
      "Epoch  3090\n",
      "Train loss  0.554767906665802\n",
      "Sample : \u001b[1mators of the truman show two qua\u001b[0mbors of nit cacancouct in tomelagass depar aullic sool he wadr anors oftroctered cortoun scinatilaty jobus on thoretre endingile\n",
      "\n",
      "\n",
      "Epoch  3100\n",
      "Train loss  0.482479453086853\n",
      "Sample : \u001b[1maud november 17 1997 no free lau\u001b[0mud nrambeing men ank hemprem alaim reaused in ilventt acticl thet an jantar of lesermaci reauplron jelo ssad st fretioned firicl\n",
      "\n",
      "\n",
      "Epoch  3110\n",
      "Train loss  0.5849331021308899\n",
      "Sample : \u001b[1mnest wrongheaded and clearly mot\u001b[0mtst fard hiskem rha collaal sures wern of cubpoant ad the new ivprsint to sut has wig ovine frobles sef cwls titslys glass offra\n",
      "\n",
      "\n",
      "Epoch  3120\n",
      "Train loss  0.9830389618873596\n",
      "Sample : \u001b[1mast names mentioned whose langua\u001b[0mss nond enkemrating indwerlpaked foredurtcenub thay glass gly whs every dane glass shidevis but dotag ty the encencextsarestyors\n",
      "\n",
      "\n",
      "Epoch  3130\n",
      "Train loss  0.6048010587692261\n",
      "Sample : \u001b[1m glass would present other elabo\u001b[0mglass wom deventing ivares becane of that ta bitssationve tam sy westo si gand is if dis anallo gat supgtore homes mest undine b\n",
      "\n",
      "\n",
      "Epoch  3140\n",
      "Train loss  0.4975626468658447\n",
      "Sample : \u001b[1men glass he was sitting in his o\u001b[0mn glass he was spriccol that the alloss net wercoras on wilenel ar icl tor by adgon y or sienest kevl tepldorteare wis sakeome s\n",
      "\n",
      "\n",
      "Epoch  3150\n",
      "Train loss  0.4977048635482788\n",
      "Sample : \u001b[1mber 2007 issue buzz bissinger is\u001b[0mer 25709use cemh t wus ingros on wath thenewram so was autedon wis te somemkivg tap stem on wes wat bebont dowo lader situfrs in\n",
      "\n",
      "\n",
      "Epoch  3160\n",
      "Train loss  1.1371867656707764\n",
      "Sample : \u001b[1mmshell where stephen glass lived\u001b[0mplaco io tim grapt an ass he casl cbll ae ne siout latain thim the bey und theor the ngs ooly monid te ssed charsat nersuibty he\n",
      "\n",
      "\n",
      "Epoch  3170\n",
      "Train loss  0.6128523349761963\n",
      "Sample : \u001b[1mother immigrant groups he though\u001b[0mther ipsifroed coopos am alasss has vorke her ow whilg as ald watcons antem te spgaot edfoct keals thetukn the stiorid ald areme\n",
      "\n",
      "\n",
      "Epoch  3180\n",
      "Train loss  0.4477089047431946\n",
      "Sample : \u001b[1mhecking during some of the perio\u001b[0me in ay autiog on heme nas lveane nfy hadryove siviratsty madis eved and and wasterthe gewrsed so the new infesticherk glas s af\n",
      "\n",
      "\n",
      "Epoch  3190\n",
      "Train loss  0.4357745945453644\n",
      "Sample : \u001b[1m salmon and listened as glass un\u001b[0msals mat tis anting as newe doficall thet hi none na atoyhorsptorkion his che litedent nome fo meattoss as toked you afked workm\n",
      "\n",
      "\n",
      "Epoch  3200\n",
      "Train loss  0.5630958080291748\n",
      "Sample : \u001b[1mtified in the story as a bigtime\u001b[0mified in the sund siten incond hid al ictuanoons for the figlas andiy in ay the wably in the forbyoaver bd toce epporedinat  all\n",
      "\n",
      "\n",
      "Epoch  3210\n",
      "Train loss  1.0854750871658325\n",
      "Sample : \u001b[1m sullivan who preceded michael k\u001b[0melponget of stuphen wescous toy chain nate magepal segredusthe compry kid goud line dame that souphe frist on makel attef the ra\n",
      "\n",
      "\n",
      "Epoch  3220\n",
      "Train loss  0.6798043847084045\n",
      "Sample : \u001b[1m a b his shit wasnt always as to\u001b[0ma b his bott wiknd hid it no sxuge unow shise and nate htiss stilgulat grass the lhows cas arsidites tho was arkudity dasllsnimn\n",
      "\n",
      "\n",
      "Epoch  3230\n",
      "Train loss  0.4810851514339447\n",
      "Sample : \u001b[1mn him involved you need to tell \u001b[0m who rnglane tam basat as thetn eponicten ind bithengin sent we coblicalon ind enication in nf exior ciepce in the kelly duste t\n",
      "\n",
      "\n",
      "Epoch  3240\n",
      "Train loss  0.4442562460899353\n",
      "Sample : \u001b[1mer 6 1997 the young and the feck\u001b[0mr s 279wor d happiciazintmeesubuth poblitth stevin antiona glissspoftinger stor and intust knd he made offficall duy ingusthonto\n",
      "\n",
      "\n",
      "Epoch  3250\n",
      "Train loss  0.4914408326148987\n",
      "Sample : \u001b[1mn but the chairperson of the boa\u001b[0m bbyit a foredber him bot de the preco celverued holowhong is the na drowhenge of hiss dinct in thp ackull and into the raypress\n",
      "\n",
      "\n",
      "Epoch  3260\n",
      "Train loss  0.9837068319320679\n",
      "Sample : \u001b[1mss academically he did extremely\u001b[0ms acenvertathered hichop nithley whot lewre fby ianfen icrupert thit haple if ling aden hean wa thn soredreporenh diminal ictupl\n",
      "\n",
      "\n",
      "Epoch  3270\n",
      "Train loss  0.8398199081420898\n",
      "Sample : \u001b[1mnsylvanian when glass became exe\u001b[0msylfalind to exw his cruticl was cllgaglo sind flasie andal whelayin and wnss intare to de ha pacasisor he kedenged and teen amo\n",
      "\n",
      "\n",
      "Epoch  3280\n",
      "Train loss  0.5626249313354492\n",
      "Sample : \u001b[1mer actually returned but in spir\u001b[0mo teanemed wobhuhe eeithis sone ag anst of grivn a wall wes in teed nid of aden gizriog aplecraid yinx ene caitecons it hal glad\n",
      "\n",
      "\n",
      "Epoch  3290\n",
      "Train loss  0.49025511741638184\n",
      "Sample : \u001b[1m trademarks at a magazine filled\u001b[0mandady tr sqyon buuzite toreate his lome fich comselusss i so corke bussensthe pittdan thas lyey ut so siod eff bidy hactwhre cl\n",
      "\n",
      "\n",
      "Epoch  3300\n",
      "Train loss  0.7426842451095581\n",
      "Sample : \u001b[1mndbreaking story on the us news \u001b[0mdor linge staringlinssive ichis ua hu the callices ly int thing in ond incouers and din accechies but be ane anite the orfitoly \n",
      "\n",
      "\n",
      "Epoch  3310\n",
      "Train loss  0.8871877789497375\n",
      "Sample : \u001b[1mpoint average at penn was hardly\u001b[0moun ablatings romicy od storthy and parf ded it also ghassishe lasdinglid hinhed an abyiches id wacerons whiss ven a mayot ha wa\n",
      "\n",
      "\n",
      "Epoch  3320\n",
      "Train loss  0.5229699611663818\n",
      "Sample : \u001b[1mrespect him and like him a lot i\u001b[0mesmews him intrghorig ont madoss of the felling in a vowf heew re abde peredes in hneainc thes yo mask onetri who sagiess hid no\n",
      "\n",
      "\n",
      "Epoch  3330\n",
      "Train loss  0.4698711335659027\n",
      "Sample : \u001b[1mprintpermissions vf media kit pr\u001b[0mrintxemin anten a purd ait bith the phead lizs sf confeedious rostooncands atte mentional stof ind buth llagns jo onvontty a ten\n",
      "\n",
      "\n",
      "Epoch  3340\n",
      "Train loss  0.46155136823654175\n",
      "Sample : \u001b[1mast names mentioned whose langua\u001b[0mpt nate incortones louthedinilyeventele yo kien crevesten a fe luags fas tom dad ho kely uppomprail alints wistrestsubed in they\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3350\n",
      "Train loss  0.503817617893219\n",
      "Sample : \u001b[1m compare with this surreal episo\u001b[0mcompacle in whethad atreyger hid inntsmemino thes silgoanl of ilesind incines tha stcknpsst an an a tulle w stinn mnvend on phen\n",
      "\n",
      "\n",
      "Epoch  3360\n",
      "Train loss  0.9937272667884827\n",
      "Sample : \u001b[1mnedy jr editor of george would w\u001b[0migg if theal wat groks l itst whackir butr ffrat dnd a toeme to ming oup chmortharstorn aven byothe satdre thager of halurgla wh\n",
      "\n",
      "\n",
      "Epoch  3370\n",
      "Train loss  0.6549186110496521\n",
      "Sample : \u001b[1mly warded off a fortune reporter\u001b[0my maad deyfaus fuastuess vedit stemo nam stove pernow id ft ris sugladp he as auted in owver kilgeais heedprace jherk hade is ex\n",
      "\n",
      "\n",
      "Epoch  3380\n",
      "Train loss  0.4370209574699402\n",
      "Sample : \u001b[1m creators of the revenge of the \u001b[0mcrattons westor apkinge he llast edithangirtou pheicas lith atte bethonallerd fathrsig and worldwe borkersut horkong lane betail\n",
      "\n",
      "\n",
      "Epoch  3390\n",
      "Train loss  0.3712543249130249\n",
      "Sample : \u001b[1mor his support of glass he offer\u001b[0mg tha somalestyes naxle toldolane and waske styesvirit at wak santhe hewprescemedmants act carepring the nalepry an the net allv\n",
      "\n",
      "\n",
      "Epoch  3400\n",
      "Train loss  0.4244239330291748\n",
      "Sample : \u001b[1metents at forbes digital tool wh\u001b[0mieg sn ty qreat curgezr g ad instary aisssenubrtimants wors be fibm chacker int deters andillege out himelu stom ghas s tidly to\n",
      "\n",
      "\n",
      "Epoch  3410\n",
      "Train loss  1.1976157426834106\n",
      "Sample : \u001b[1more aware of not only your own c\u001b[0mllemmanc by adclunge ho keqy tivh miske shoubditarisp butng the hiver detarngrttto spoil that hy bagonstres it ta onexislly llfe\n",
      "\n",
      "\n",
      "Epoch  3420\n",
      "Train loss  0.6738563179969788\n",
      "Sample : \u001b[1mnto the big leagues when confron\u001b[0mto bit wince bliss aig in in aud thecovesicurenti hor mrnat dousth mererzs ardayle repting not lake ner a sutt che orfig the cho\n",
      "\n",
      "\n",
      "Epoch  3430\n",
      "Train loss  0.5759599804878235\n",
      "Sample : \u001b[1mso why wouldnt he figure out wha\u001b[0myouher amoansy ppedanus jo kleme clas stohe woy anwerkios hedors tri thene ho sfes if highland andi goon wive sale himh alter ut\n",
      "\n",
      "\n",
      "Epoch  3440\n",
      "Train loss  0.4471393823623657\n",
      "Sample : \u001b[1mked hard to succeed jeffrey glas\u001b[0mm ghass is iferaed ditgitilguthi galinm ats thiy of ackudly inverobal art iste leps inkl st m to lreenes impisoungn whagt as bli\n",
      "\n",
      "\n",
      "Epoch  3450\n",
      "Train loss  0.42951589822769165\n",
      "Sample : \u001b[1m with precocious kids he was far\u001b[0mwith ande thes gegrs the celvobuling inhrsdittin ancaucher of the kelandond tik of clling appord ckela yeno ind im ntent of ited\n",
      "\n",
      "\n",
      "Epoch  3460\n",
      "Train loss  0.8187257647514343\n",
      "Sample : \u001b[1m sidewalk and on a weekday in ju\u001b[0msldewall all onhe alouspact yyouss galist dibleres lew le at the hey and scinore hild in waurntharsin the mant enermatr of litil\n",
      "\n",
      "\n",
      "Epoch  3470\n",
      "Train loss  0.7459855079650879\n",
      "Sample : \u001b[1m as the best fact checker said a\u001b[0mrsstoa in instouslvintmi stithurn wasmag tan smeites hed to heat huss as why tee ngity ht hesss tnat on tho natorlarsty hached i\n",
      "\n",
      "\n",
      "Epoch  3480\n",
      "Train loss  0.5443524718284607\n",
      "Sample : \u001b[1mldbe parent surrogates wanted on\u001b[0mdoro kanly hadd inte esmintins invangits he stod pary uand secordly rule boon that had ans rutacle lobes bub th of enoto se bee \n",
      "\n",
      "\n",
      "Epoch  3490\n",
      "Train loss  0.40064772963523865\n",
      "Sample : \u001b[1mto a fault and is sweet and soli\u001b[0mo a far a deep hewerdeatled that in thas a commers if theck icsussh whay stacclaited domet at he keldow alaumen and cittl ke hi \n",
      "\n",
      "\n",
      "Epoch  3500\n",
      "Train loss  0.39163053035736084\n",
      "Sample : \u001b[1m props not that many years later\u001b[0mppord to ha listuaconupors aiterkull ty bat benmes hil ols megling the lavel gly st limse camaty mas bigtren lits scrobr warm ay\n",
      "\n",
      "\n",
      "Epoch  3510\n",
      "Train loss  0.8420974612236023\n",
      "Sample : \u001b[1mich glass had created on the com\u001b[0mfr glass had created ar thi noberial jey her luck leff reas st pabr tor chure mace and hinges stid theredibes dethre canedd fara\n",
      "\n",
      "\n",
      "Epoch  3520\n",
      "Train loss  0.8686742782592773\n",
      "Sample : \u001b[1mterials when factchecking the st\u001b[0menialw wast was in maica is a futhin stertinnamal ortountrye unitisss abouteruay jafrs ient bpuring a qaitte a denty th to sovea\n",
      "\n",
      "\n",
      "Epoch  3530\n",
      "Train loss  0.4988479018211365\n",
      "Sample : \u001b[1menes and characters this time pa\u001b[0mnes and diviater yy mas wattl glevely asm crumayy on gig aste pactlat jnoo andon ofte thats wob if ensoot hishe wabliyy and catr\n",
      "\n",
      "\n",
      "Epoch  3540\n",
      "Train loss  0.38646429777145386\n",
      "Sample : \u001b[1my complex scenes and also becaus\u001b[0m mostion buthivet an the nowsed nhint a tovder of all hag pane dent me sibpesibg ins andultions acke weter snited aus sninaty ho\n",
      "\n",
      "\n",
      "Epoch  3550\n",
      "Train loss  0.3747890293598175\n",
      "Sample : \u001b[1mng no address or the account was\u001b[0mn ffor awied was fur icrose whet to kelffist has pare lyot ehetec anctyellmasha d sare us the ntimh the efone he keve blisite to\n",
      "\n",
      "\n",
      "Epoch  3560\n",
      "Train loss  0.4516751766204834\n",
      "Sample : \u001b[1mthe night before michael had lef\u001b[0mhe laght cewhin ambyli pracupsoc efacul magitan wath i was dyis mabule mewrino note w teel se exmy inveruple sola get as byyn ai\n",
      "\n",
      "\n",
      "Epoch  3570\n",
      "Train loss  1.2816215753555298\n",
      "Sample : \u001b[1mother whose accomplishments in h\u001b[0mich ag asse was cheed tare dend to uadr to thate wore ol icsimila aldiory bos af ghas vided in aeders a butrackev ctomed io foct\n",
      "\n",
      "\n",
      "Epoch  3580\n",
      "Train loss  0.9186334609985352\n",
      "Sample : \u001b[1mrtin peretz himself peretz had s\u001b[0mlin chelgn wis rat kited to the olleatied homeven pract at ue ssurendra pelyongl sive atked bbezine dent whan hess wething phamb\n",
      "\n",
      "\n",
      "Epoch  3590\n",
      "Train loss  0.5742887258529663\n",
      "Sample : \u001b[1mlass however had actually worked\u001b[0mate homenne nae repamles fow is entem that glass whit his abpicolot scuapls net faret of surdis wht morousn me thithe dewnolig t\n",
      "\n",
      "\n",
      "Epoch  3600\n",
      "Train loss  0.3917773365974426\n",
      "Sample : \u001b[1mde the issue careers digital edi\u001b[0me the condoug tand af bhe to dible if menit lote he had nowesian wish andy ands with and perffous upenverunget ther hed soofion \n",
      "\n",
      "\n",
      "Epoch  3610\n",
      "Train loss  0.3442689776420593\n",
      "Sample : \u001b[1mning to glass glass and lane wer\u001b[0ming to quait jaun shad the pord hagd so had yeorng kied to begoks w searse andiclackerrdictted so stom at rephyle me the callaci\n",
      "\n",
      "\n",
      "Epoch  3620\n",
      "Train loss  0.408710241317749\n",
      "Sample : \u001b[1mty that was so rich that to susp\u001b[0my that whis  and lenkeespoostayturngly shew the sravints a war campionimme thryecaline at was cralicalyoft us tohe sy in the int\n",
      "\n",
      "\n",
      "Epoch  3630\n",
      "Train loss  0.860984742641449\n",
      "Sample : \u001b[1mve and selfdestructive and if th\u001b[0me and news epe uckpoled on washe the inristoons ingut your and of hecemrea saceparexy cunltrof a atry and parepuancs vers ictlo \n",
      "\n",
      "\n",
      "Epoch  3640\n",
      "Train loss  0.6344144344329834\n",
      "Sample : \u001b[1mng glass said incessantly the sl\u001b[0me glass said chancotil tho to cenf is syepnom rian an the diper thit in in istontant aus thack an he hod to sotepo prof chavly a\n",
      "\n",
      "\n",
      "Epoch  3650\n",
      "Train loss  0.48950108885765076\n",
      "Sample : \u001b[1mn but the chairperson of the boa\u001b[0m but tho gyvirsedien cane s to the mist why it tem ndmenon orgon of ay that ond theme had turnctuch but in aat lane wount pundin\n",
      "\n",
      "\n",
      "Epoch  3660\n",
      "Train loss  0.41850176453590393\n",
      "Sample : \u001b[1mn november 1996 michael kelly th\u001b[0m tov anither who prepe thats of ropastored hiswlispleandines had been ably if whet kells and himpes s pared interes gll stil and\n",
      "\n",
      "\n",
      "Epoch  3670\n",
      "Train loss  1.6919467449188232\n",
      "Sample : \u001b[1mthe fact checkers came to trust \u001b[0mis eafrer alater in ans stok os afforre naw the venckolyg riol at war to wnic reanolagid saod carcess is llykenily onn lake sain\n",
      "\n",
      "\n",
      "Epoch  3680\n",
      "Train loss  0.6699422597885132\n",
      "Sample : \u001b[1mg before kindergarten and were c\u001b[0m metabe cunderttnant that aishe puinglows recite undsdevidite forges i kenvinitalyed whe aps maditoth a y0ur thandyjrigas s teal\n",
      "\n",
      "\n",
      "Epoch  3690\n",
      "Train loss  0.4105318486690521\n",
      "Sample : \u001b[1med for kelly and the tone he see\u001b[0md homsyonde lazed wssaglamy ut lady hoa was athin ispee kive oftryowy ct ce had syon buscondd gend he he plece h wayent pioneler\n",
      "\n",
      "\n",
      "Epoch  3700\n",
      "Train loss  0.35656726360321045\n",
      "Sample : \u001b[1migh he was a star of a school pl\u001b[0mgh hy was a sean flomssly at vitten slovem pontectee hy whess tent youk onemimpergechas a betcand hiag swhat foing adwer anm hin\n",
      "\n",
      "\n",
      "Epoch  3710\n",
      "Train loss  0.35360419750213623\n",
      "Sample : \u001b[1mother immigrant groups he though\u001b[0minteve tilleng fforden an whickmine had geblegt in the bect ac nor of wrypus t befr uit bitwasery jfoth tinc mpes bett of stqpea\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3720\n",
      "Train loss  1.0624154806137085\n",
      "Sample : \u001b[1mlf was trying to destroy what in\u001b[0my was arsuchs od ilpeasllars the meds llivel no lits him kot nas naws iand insanusat doyed ot hid nat hives the naneranist nagne\n",
      "\n",
      "\n",
      "Epoch  3730\n",
      "Train loss  0.889449954032898\n",
      "Sample : \u001b[1ma meeting at a college in which \u001b[0m aroling hiss preptim an up ro coucestoo sha spurted ant hienfresing an woute himed ins lvaly meng hiss wasllying the corver agl\n",
      "\n",
      "\n",
      "Epoch  3740\n",
      "Train loss  0.4307490289211273\n",
      "Sample : \u001b[1mr 22 1997 anatomy of a policy fr\u001b[0m 24 199a rarontst has pare fffewa coullef rata ware ushin speroncentihes wink lame thac so wincen flex sther and prent oll in da\n",
      "\n",
      "\n",
      "Epoch  3750\n",
      "Train loss  0.3450048267841339\n",
      "Sample : \u001b[1ms exceptionally good he didnt kn\u001b[0m exceptionally wool ja t timyeaply adilyet for watrones a magitate chica ofll fird ate would tild ior makilar anglagy spteansong\n",
      "\n",
      "\n",
      "Epoch  3760\n",
      "Train loss  0.3441157341003418\n",
      "Sample : \u001b[1m most gripping part of the story\u001b[0merspes withy thlys dnitns factuphol to hat woull wayling crlverucl of the lomay 1ucholl vithend wh br alyichid collon of the sou\n",
      "\n",
      "\n",
      "Epoch  3770\n",
      "Train loss  0.385138601064682\n",
      "Sample : \u001b[1min a phone conversation was this\u001b[0mn a mumiso ad a terne exmeas ande omeat bo je oletic sere that te had wrotion wrict eviribulw ie iv lay in teetaol your tived ha\n",
      "\n",
      "\n",
      "Epoch  3780\n",
      "Train loss  1.05538809299469\n",
      "Sample : \u001b[1m as answering the phone answerin\u001b[0mat anspered in tros awd y a nated an therenw yorencantid thatref he hect sca iccullazdet veeren glass sorversceeventiva pacitod \n",
      "\n",
      "\n",
      "Epoch  3790\n",
      "Train loss  0.6235066652297974\n",
      "Sample : \u001b[1mad done every name every company\u001b[0mdndount tun tout flomeh tow ungin whe alo gxoss arkilg te famazt endint defond atrvely deatalis ycurranosuentsor hiss of parlics\n",
      "\n",
      "\n",
      "Epoch  3800\n",
      "Train loss  0.40467846393585205\n",
      "Sample : \u001b[1man imaginary company glass had w\u001b[0mh lakinasis ffarina bet will y lfoore 3nos antinged in ining waskiny and saglory hom liste spo ic is 1ask hnens or  fornof aithe\n",
      "\n",
      "\n",
      "Epoch  3810\n",
      "Train loss  0.35681474208831787\n",
      "Sample : \u001b[1mmiliated her the piece almost en\u001b[0miliated hed the rickeds ind nist a siduned lan blenken wrottyen as neerlayedpitt ki had ctathe fas sctevi ahd sthe prest chacksl\n",
      "\n",
      "\n",
      "Epoch  3820\n",
      "Train loss  0.44472452998161316\n",
      "Sample : \u001b[1me and then somebody came along w\u001b[0mhing shones allsts storyeduna wores urctuen lits it fabd thet sie off selfonte l of the men y af in the kinge sunother fut rovet\n",
      "\n",
      "\n",
      "Epoch  3830\n",
      "Train loss  0.9428772926330566\n",
      "Sample : \u001b[1me boy had won over the world of \u001b[0memaonth ppent ipben the us yo rattings asemalle coufhed the ricous in thot in orsuprr ontantst his eree nf hiespucepurssting no \n",
      "\n",
      "\n",
      "Epoch  3840\n",
      "Train loss  0.5162785649299622\n",
      "Sample : \u001b[1mhich glass had concocted his att\u001b[0miche lan swe a perepiim discreat and whter cont hemd himpouspasert then ae had soungl melt aner conferene intssexesy do glived d\n",
      "\n",
      "\n",
      "Epoch  3850\n",
      "Train loss  0.3641165494918823\n",
      "Sample : \u001b[1m being replaced by cabbies from \u001b[0mbein wrexuol gas wrot ande af clucke suid thed hom rok the debretad wn the whe ghimisis an stilg his fotm a ladecuins recometha \n",
      "\n",
      "\n",
      "Epoch  3860\n",
      "Train loss  0.3126469850540161\n",
      "Sample : \u001b[1mexposing glasss fabrications he \u001b[0mqpising gloss formokt untsy keplliatled kellm non betual ey s tale kild olfteg tomby nlves ti suecumir sanding ornial stephem he\n",
      "\n",
      "\n",
      "Epoch  3870\n",
      "Train loss  0.3451378047466278\n",
      "Sample : \u001b[1mthrough highland park high in ad\u001b[0m raveimi sood a makip llag that of anst of the mrifath lite him offtie abl omre firs lyonl tin speridus therefirnoly en thes sel\n",
      "\n",
      "\n",
      "Epoch  3880\n",
      "Train loss  0.4980771243572235\n",
      "Sample : \u001b[1mive journalism were consciously \u001b[0min eviry her watt rs anuistintshe cpait fact beece enary hig glass the sham qloon inallatsis sinh ehows leretrenasi mancro sarti\n",
      "\n",
      "\n",
      "Epoch  3890\n",
      "Train loss  1.0933140516281128\n",
      "Sample : \u001b[1msing his parents and bettering t\u001b[0midd hasspore chollow had chel nte ther glass was holly taed ape matilivanss uake ald whiness net on the bemes in ad itrent doot \n",
      "\n",
      "\n",
      "Epoch  3900\n",
      "Train loss  0.5919530391693115\n",
      "Sample : \u001b[1mlling glass to knock it off but \u001b[0mling rstin to exper ilway and ivire apshareding anvergal the qaict cfer inad hears and tay bectupe fat and an of youtiveacke a d\n",
      "\n",
      "\n",
      "Epoch  3910\n",
      "Train loss  0.35520899295806885\n",
      "Sample : \u001b[1ms conference had not taken place\u001b[0m concented late the preat that ever este ha docat lane nemlos af dextom an fory chould apxes the nig hand or pas anss farltoys a\n",
      "\n",
      "\n",
      "Epoch  3920\n",
      "Train loss  0.31725412607192993\n",
      "Sample : \u001b[1mglasss honda leaving the new rep\u001b[0mlisss heldrs atilisg noot mo nacous foret whe the epleingling neten itnly geck picciblus sod the mang mik att daictter whourea e\n",
      "\n",
      "\n",
      "Epoch  3930\n",
      "Train loss  0.32210710644721985\n",
      "Sample : \u001b[1m ran in the magazines october 16\u001b[0mran in the magaithe lsperfance of the kyonf lvee so sim fats he andy that inspeceftimner atl of and hime of themigls w yllus lig\n",
      "\n",
      "\n",
      "Epoch  3940\n",
      "Train loss  0.8055247068405151\n",
      "Sample : \u001b[1md been relatively short but mich\u001b[0m viened andidith shige thiner glasss deacons at soim a topto him as onditre it phese in in hidinssoved in wain thail whr toke ma\n",
      "\n",
      "\n",
      "Epoch  3950\n",
      "Train loss  0.8195394277572632\n",
      "Sample : \u001b[1m the office gossip he knew every\u001b[0mthe oxgiat efire him thing thate nas illes quent on whot no on icstimnzin magaz us tho stomedis scaifcaly urops tot as give sy l\n",
      "\n",
      "\n",
      "Epoch  3960\n",
      "Train loss  0.46001723408699036\n",
      "Sample : \u001b[1mk place look youre not backing m\u001b[0m plasn jaur wathe nepdricting resurici ard simpor onns upen eo him we corde to khe recondine wo ld exioual out oy broowhing enfl\n",
      "\n",
      "\n",
      "Epoch  3970\n",
      "Train loss  0.31749922037124634\n",
      "Sample : \u001b[1mrious accounts he held his own a\u001b[0mious accoules no  githoon wabecinus ffordervsn infedent criatilystemt an t ous ind that if halwerew andi glasmphy freatereduare \n",
      "\n",
      "\n",
      "Epoch  3980\n",
      "Train loss  0.3141821026802063\n",
      "Sample : \u001b[1mn inserted fake mistakes into hi\u001b[0m insilase foon knived ditwis fromadelly yof meleang that wat foubsof syon qiolesing his gentlpirded fwes and in hie gines imtial\n",
      "\n",
      "\n",
      "Epoch  3990\n",
      "Train loss  0.31044963002204895\n",
      "Sample : \u001b[1mael kelly chuck lanes predecesso\u001b[0mel usthy haversts he repartcalper whve hisserfas acter carater an whr oac andose nat repartilleest yew tha sild mas bay stersem \n",
      "\n",
      "\n",
      "Epoch  4000\n",
      "Train loss  1.2713426351547241\n",
      "Sample : \u001b[1may presented by the nationally r\u001b[0mss raskligg chackend inconded thas ithendes tpotter te lake shod the nstins is chuck in glass deld cabdet to on ply enversilat p\n",
      "\n",
      "\n",
      "Epoch  4010\n",
      "Train loss  0.6651317477226257\n",
      "Sample : \u001b[1mced that someone had signed in t\u001b[0med that sourent of sexboof dole foredracturazin wind it leer was and his arolly shioss ande atorhed in urmanestime by ifly fabic\n",
      "\n",
      "\n",
      "Epoch  4020\n",
      "Train loss  0.3771234154701233\n",
      "Sample : \u001b[1morge rolling stone the new york \u001b[0mrye rioute s oftes invel asthiey tep kneld he was an ins mimy yo bluses and mokea letht geade spubtey hisplexk ne the frittis va\n",
      "\n",
      "\n",
      "Epoch  4030\n",
      "Train loss  0.31826120615005493\n",
      "Sample : \u001b[1molden boy everybodys brother eve\u001b[0mlden hinseruliavons siorce se stomed the framdenis anguitn jas lane n the priss ass andy hadrisacand himsely belut and he ande t\n",
      "\n",
      "\n",
      "Epoch  4040\n",
      "Train loss  0.34606704115867615\n",
      "Sample : \u001b[1meople out he was always pissing \u001b[0muare tuit tha frgazed ss and weet bustim and intreate nom hichntortonsm int recorderd wastor warkens lone ntad merely htrggo ste\n",
      "\n",
      "\n",
      "Epoch  4050\n",
      "Train loss  0.5703095197677612\n",
      "Sample : \u001b[1mareers he was almost brutally se\u001b[0mregray w stons llay siggathal ainglisst peov luag iface tad caracols mactopl of newewn astereatalngy frapalo ny ulads manexissia\n",
      "\n",
      "\n",
      "Epoch  4060\n",
      "Train loss  1.077563762664795\n",
      "Sample : \u001b[1m a b his shit wasnt always as to\u001b[0mr m hes sime forber canisively gnende nimsty stor disterestore andivert and  oo hie not to brew thaf bets reparity and retabes a\n",
      "\n",
      "\n",
      "Epoch  4070\n",
      "Train loss  0.6514614224433899\n",
      "Sample : \u001b[1mn who assumed he was gay later o\u001b[0m who assreple pirguqus in the oolly andeuter edicroot hidht wastin alld inchalsstten a tes mokedratuaked of erstian the dofer bu\n",
      "\n",
      "\n",
      "Epoch  4080\n",
      "Train loss  0.3790283799171448\n",
      "Sample : \u001b[1mntrue stories that he could basi\u001b[0mtolee glass fof dis  oo nis ittis add frem at ofe n wook dimkte stors actio suated whthaviroel aspes thacunt anongy eniteqsi th \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4090\n",
      "Train loss  0.30441445112228394\n",
      "Sample : \u001b[1m because he had been a kelly dis\u001b[0mpeca in the ot bletich ap the ixstarolas sho labker a walled on actuditim and adenobyeniss wo jo alaimst th tho lived his fon wa\n",
      "\n",
      "\n",
      "Epoch  4100\n",
      "Train loss  0.2782309651374817\n",
      "Sample : \u001b[1mnity fair joan didion and john d\u001b[0mity fair foav diding ind then glassscheas be way ins and in seeaninesslebitich yr alazowesy dinstont acpusand x came thew yfa me\n",
      "\n",
      "\n",
      "Epoch  4110\n",
      "Train loss  0.2993999421596527\n",
      "Sample : \u001b[1mo be fair and honorable was symp\u001b[0m be parradg nit at the eyond touedvor and herrees hed seingl shad theseg that glada nountedigg and woqeo kenss thes he vrenes ap\n",
      "\n",
      "\n",
      "Epoch  4120\n",
      "Train loss  1.7234596014022827\n",
      "Sample : \u001b[1mass his colleagues didnt know at\u001b[0mss the mugeplsedn whict g orss upep yed soof adree and s enerrepoknt the stlercopaeat ofed niv ghat heskened it aslervaccatly fa\n",
      "\n",
      "\n",
      "Epoch  4130\n",
      "Train loss  0.7712311148643494\n",
      "Sample : \u001b[1m highland park is a town of boys\u001b[0mumpoon exptotnemies ind tratiz header smouce that glass whule on the new remugtithe wagllegleds if camebue degatly onspresting r\n",
      "\n",
      "\n",
      "Epoch  4140\n",
      "Train loss  0.38809555768966675\n",
      "Sample : \u001b[1mss whose stories had attracted t\u001b[0ms mowa astaicl anga issanalis seewnous his anfarrigarests af aumpount insmaniburit ando hiss after corcabue tun tsyor peed in ad\n",
      "\n",
      "\n",
      "Epoch  4150\n",
      "Train loss  0.32646486163139343\n",
      "Sample : \u001b[1mrheads memos faxes and phone num\u001b[0mheals nfar stody at enstinn wores creckive und that chea a ficed lite nem dobe to hy wrs cand thettel ed tor owhy mishomter ever\n",
      "\n",
      "\n",
      "Epoch  4160\n",
      "Train loss  0.3173767924308777\n",
      "Sample : \u001b[1ms best and brightest he was vote\u001b[0m best and ritale the work on then to the himper thagsingle stondghorty as instepla ingossli adno trowhe dod tom and gappss ind s\n",
      "\n",
      "\n",
      "Epoch  4170\n",
      "Train loss  0.4044691324234009\n",
      "Sample : \u001b[1mool in the role of a jukt micron\u001b[0mous hersestinge pllmigat foelin medot by the corepebticl wance alwatovhing chacker atend in inventic thas froblethine sy ufrit a\n",
      "\n",
      "\n",
      "Epoch  4180\n",
      "Train loss  0.9742764234542847\n",
      "Sample : \u001b[1mmaureen orth olivia de havilland\u001b[0mapare to hishorgaznvels mrenum chatsew whe patsente had wonk nage alasy co pare inthim nlais lastonke lackisain but new yow brtt\n",
      "\n",
      "\n",
      "Epoch  4190\n",
      "Train loss  0.5887306928634644\n",
      "Sample : \u001b[1mare willing to smear someones pr\u001b[0mre filling tol whas st ahen a mblic indpur buta and readrest dithem gle igulenon vercenbed eroske souf dive thy whole they wale \n",
      "\n",
      "\n",
      "Epoch  4200\n",
      "Train loss  0.3810001611709595\n",
      "Sample : \u001b[1mntial to him in 1997 despite his\u001b[0mtial touth ar westonsm becis tos nat oth ned to mone reptire and hos hal fors thed iv rigkalllave to iter in the nen blus ined h\n",
      "\n",
      "\n",
      "Epoch  4210\n",
      "Train loss  0.32271334528923035\n",
      "Sample : \u001b[1m closedwhatever who are the peop\u001b[0mclosedphatentt ss afof thes wureh is menivets id ueraumo t asex beviliggly taly backeas puttabl callitsa de appontithen wlass wa\n",
      "\n",
      "\n",
      "Epoch  4220\n",
      "Train loss  0.2913654148578644\n",
      "Sample : \u001b[1ming psychology at stanford and a\u001b[0mnd hade rsod hos rencimed is uadr ancinaretthoo sos verenow in ampareccount you bou divly for taven a ters he was dutente a lext\n",
      "\n",
      "\n",
      "Epoch  4230\n",
      "Train loss  0.30657756328582764\n",
      "Sample : \u001b[1msac curved in the shape of a cla\u001b[0mod condelnem the velieg a tellugg astinctiry numpkexad ais seed of thito bet of reastheakrhag ansinglans ghass driat lly sho dik\n",
      "\n",
      "\n",
      "Epoch  4240\n",
      "Train loss  1.4304673671722412\n",
      "Sample : \u001b[1mpes of wrath at highland park hi\u001b[0mns an wlate hore brition and ie is dilm in thisn qroal coleedu gets wninf eritin mirnofically ke destees if all wvenonions thet \n",
      "\n",
      "\n",
      "Epoch  4250\n",
      "Train loss  0.5584976673126221\n",
      "Sample : \u001b[1mb site so i dont know how easy o\u001b[0m site to snooft hewh tspe the glast hemere thet mon mant on th thite rugetalpont is keols butwion we haise un thet no ndafo thes\n",
      "\n",
      "\n",
      "Epoch  4260\n",
      "Train loss  0.40103667974472046\n",
      "Sample : \u001b[1moment lane thought about just te\u001b[0m in 1urhidg aslis water onor eard recibal derwore urtadured thety ungnere whylress parey blsmeh mo enakels glass campeactopt ind\n",
      "\n",
      "\n",
      "Epoch  4270\n",
      "Train loss  0.28471648693084717\n",
      "Sample : \u001b[1mof the hyatt they went into the \u001b[0mgithag pungespun glaess in mwsek ked suladily defored of be ald he ore made of wion ave ceveres af dverarked whe hig issedring a\n",
      "\n",
      "\n",
      "Epoch  4280\n",
      "Train loss  0.2509627044200897\n",
      "Sample : \u001b[1mg correspondence and an occasion\u001b[0m corresurina je the tely harkedla beln oully was poono ty uer thes in glass yourdoon tory wald it pen that plehe bhey ind serect\n",
      "\n",
      "\n",
      "Epoch  4290\n",
      "Train loss  0.27029743790626526\n",
      "Sample : \u001b[1mass had a brother who lived in p\u001b[0mss had a cabiccho was silderit badinit workeig the wis th un iconamie atrle madsongupr of the nea allfurnd ioconstoke aymedfing \n",
      "\n",
      "\n",
      "Epoch  4300\n",
      "Train loss  0.39257413148880005\n",
      "Sample : \u001b[1mhonest in approach that one cant\u001b[0molest wesh pnal achtory aid oftzre chepre adma chulasy and iverals frim thing abulay had tons andem in bourd bizireds is dimbati\n",
      "\n",
      "\n",
      "Epoch  4310\n",
      "Train loss  1.2979286909103394\n",
      "Sample : \u001b[1m that he had asked his brother t\u001b[0mthatye had wretten with ately tels long as thern wath now hiss ont lene a lated in and andy hivemila beds reen prondy hadd hadde\n",
      "\n",
      "\n",
      "Epoch  4320\n",
      "Train loss  0.519389271736145\n",
      "Sample : \u001b[1mctober 20 1997 cheap suits octob\u001b[0mtober 10 1997 dlaf ctote a sectlerthes kell wasted ow was colime the ombstravertiod tas corkucand a shemagr es gloss had eerno t\n",
      "\n",
      "\n",
      "Epoch  4330\n",
      "Train loss  0.3322597146034241\n",
      "Sample : \u001b[1mched from a table in the far cor\u001b[0mnew of hem realst in to burcoust yourge was siggesns whthe vany yanere chicacinared andty tun alat hackere ungald it per aus sio\n",
      "\n",
      "\n",
      "Epoch  4340\n",
      "Train loss  0.27191200852394104\n",
      "Sample : \u001b[1m97 a fine mess april 21 1997 the\u001b[0m7 wasmed noce berid th ilsichee sy siew and sting to poofles of st the remay hi sod anmbyif absic beraurally gevor and hea magnt\n",
      "\n",
      "\n",
      "Epoch  4350\n",
      "Train loss  0.2527168393135071\n",
      "Sample : \u001b[1mhe reported that while the hotel\u001b[0me vermagra tue woorcolved ktokn withanisy was 2f that he had wastongland nnto kelubaice ad icticllicit ondon  a aron ous hightal\n",
      "\n",
      "\n",
      "Epoch  4360\n",
      "Train loss  0.27796414494514465\n",
      "Sample : \u001b[1mconvention featuring trinkets re\u001b[0monationslys gatile apcousity is tour ham no freme rev if eney cklldat to enor and there so he was im in outrealterhers glass qai\n",
      "\n",
      "\n",
      "Epoch  4370\n",
      "Train loss  1.5429997444152832\n",
      "Sample : \u001b[1md yet i was in this chair and ia\u001b[0m oft jo us ge tad buckenle ken yoveny the men im kely he glys depir indensitis spere hi his rimeftidn they in ableitn sner prand\n",
      "\n",
      "\n",
      "Epoch  4380\n",
      "Train loss  0.5393841862678528\n",
      "Sample : \u001b[1m of colleges and universities su\u001b[0mof college its donennc glant to ackios in the suriees a tel unca prous af the leniense hal yong the eftors the chice vanderor a \n",
      "\n",
      "\n",
      "Epoch  4390\n",
      "Train loss  0.3182617425918579\n",
      "Sample : \u001b[1mprintpermissions vf media kit pr\u001b[0mrenclewngl aten fuep a into hid kingfots jott moni glassa das ware fane by andeting tees at onfrivivinist ofs aske sheepritedant\n",
      "\n",
      "\n",
      "Epoch  4400\n",
      "Train loss  0.2557034492492676\n",
      "Sample : \u001b[1mdy to lend a sympathetic ear to \u001b[0my to actore sterleds yo averator the ever sfteatten teotuluse dosld bleation to nat ceorian silway fration an soutn fore rede ch\n",
      "\n",
      "\n",
      "Epoch  4410\n",
      "Train loss  0.25755777955055237\n",
      "Sample : \u001b[1mntly boston globe columnist patr\u001b[0mt th tohen emeat on inne dis seas to besm thas dived thetreys assd dimexestuc tane ded thede snovet depe tht in ver unt it ane i\n",
      "\n",
      "\n",
      "Epoch  4420\n",
      "Train loss  0.8879086971282959\n",
      "Sample : \u001b[1m the hyatt lane and glass headed\u001b[0mtar wyone tope ald hishedriats hext lane a mitherit ontsearoly exveadito stivett the victuble epnitos ofr conteres istembing gan\n",
      "\n",
      "\n",
      "Epoch  4430\n",
      "Train loss  0.7265143394470215\n",
      "Sample : \u001b[1mess diana would look like if she\u001b[0mss dianr worez lowe lefer and inca wratsck lags inte rematler som glats he was maked is blea ferustikily hisstlvedig te sada jac\n",
      "\n",
      "\n",
      "Epoch  4440\n",
      "Train loss  0.41279518604278564\n",
      "Sample : \u001b[1mng shots from his possible doubt\u001b[0mg thot simeeot reseafaling athit fay boucm the eallering troted has s extey dean whit owno is gaven awdeanul hadedonsterfatre un\n",
      "\n",
      "\n",
      "Epoch  4450\n",
      "Train loss  0.2788577973842621\n",
      "Sample : \u001b[1me editor of the new republic aft\u001b[0m editor of the new republic after thot stereve his amber himblist ande ithiescanllyoaten ymer a waththis ald then come madinom q\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4460\n",
      "Train loss  0.23614192008972168\n",
      "Sample : \u001b[1m his skill at creating incredibl\u001b[0mhod sussecos souding wit magafis hit subees fristimed s pemedy a pond hied if ibeto  oullimade wasthit an 3ikulliss fear hanng a\n",
      "\n",
      "\n",
      "Epoch  4470\n",
      "Train loss  0.23785078525543213\n",
      "Sample : \u001b[1m central avenue the shopping are\u001b[0mxestral avanofy had spemace theckente thly nevemolt as incend laberre hab se forl hid lavedid atriver wan wes and hig rine if ch\n",
      "\n",
      "\n",
      "Epoch  4480\n",
      "Train loss  0.7290827035903931\n",
      "Sample : \u001b[1min 1994 and went to work for the\u001b[0mn 1995warcharents akely nerrant of rope n were repoupny ertaree but lidnon ouk smiondapmanacipqolingear wha d himeric illod 2ube\n",
      "\n",
      "\n",
      "Epoch  4490\n",
      "Train loss  1.0253806114196777\n",
      "Sample : \u001b[1mans imagination but there wasnt \u001b[0mhs hamagy thes fod oronge thess off birked ghe hinecrlered and youre madexils mphalperneg and wiicrisectiols acffiricing the sta\n",
      "\n",
      "\n",
      "Epoch  4500\n",
      "Train loss  0.4436316192150116\n",
      "Sample : \u001b[1m as stephen glass spun feverishl\u001b[0miss deily dext e tobler cotsers it was in tory uadry to ctlly und oover he was maryctrop limse na manase fat jerssitedy betas ff\n",
      "\n",
      "\n",
      "Epoch  4510\n",
      "Train loss  0.2914488911628723\n",
      "Sample : \u001b[1mg womens magazines he mentioned \u001b[0m womens migagaloses dat ansthes wont of tats the recamptrof but nisuse fir boun suppots yo eville inforutinaking oremen it abret\n",
      "\n",
      "\n",
      "Epoch  4520\n",
      "Train loss  0.2360019087791443\n",
      "Sample : \u001b[1m something better more colorful \u001b[0msosstigls anous evince of the blyows as alass efarchored evaret of storker alssind nems thutth gore nichienthes dithote lana wo \n",
      "\n",
      "\n",
      "Epoch  4530\n",
      "Train loss  0.22478583455085754\n",
      "Sample : \u001b[1mbiz foroohar from the online pub\u001b[0miz for aboz sois his llasls wrthin hlaky aprot at jour atm y junceming roo tell we anders dobgrinisll dotioks kely halde courrak\n",
      "\n",
      "\n",
      "Epoch  4540\n",
      "Train loss  0.4225741922855377\n",
      "Sample : \u001b[1mr staffer glasss pieces lost som\u001b[0m stalden wers improali pay ty hf cerlmsy butkne me was hal f crock acl scouler anderribsin torbor hunhe warkenvenrinerive eduris\n",
      "\n",
      "\n",
      "Epoch  4550\n",
      "Train loss  1.010257363319397\n",
      "Sample : \u001b[1metracing these imaginary steps o\u001b[0mcharreled the migiane it ub that and lmen inctochar an ow hax laci dexar a eruburkmiss what pening chitry anvinatips to in ingia\n",
      "\n",
      "\n",
      "Epoch  4560\n",
      "Train loss  0.4281795024871826\n",
      "Sample : \u001b[1mb site so i dont know how easy o\u001b[0m himed of sements an arkublyo groos esed latel gless iner ham nof her gotseasn anjucthe wosten mouden bugim a d toruot colvered \n",
      "\n",
      "\n",
      "Epoch  4570\n",
      "Train loss  0.2957579791545868\n",
      "Sample : \u001b[1mer a story he had written about \u001b[0mr a garaye kiod of ptonk fof the ernizaledion an arnatim ofthr uichaol difl  forl aning and inecusthom wist of thion into tuther\n",
      "\n",
      "\n",
      "Epoch  4580\n",
      "Train loss  0.2665981352329254\n",
      "Sample : \u001b[1m someone comes along who appears\u001b[0mtowwike dobesialasis warcourd tle whal devint in whe poodd an thit blur ferreyong r bithmens in syseon efferther a maglake haded\n",
      "\n",
      "\n",
      "Epoch  4590\n",
      "Train loss  0.28250768780708313\n",
      "Sample : \u001b[1meenwriters when unmasked stephen\u001b[0mededshing asho grupthed issirgibly unrwast is verdicl evinger the colleas deat in in aft onev comsed int soivs on that cales ont\n",
      "\n",
      "\n",
      "Epoch  4600\n",
      "Train loss  0.2845907211303711\n",
      "Sample : \u001b[1mling his own brother into his fa\u001b[0ming his off cllica lant his atoeh kell heddsntningibus docloasted cluckie  buling ast geare whit a naple byuther you had ye the \n",
      "\n",
      "\n",
      "Epoch  4610\n",
      "Train loss  1.5475175380706787\n",
      "Sample : \u001b[1mnd we sat here for a little whil\u001b[0me her ator uy hetrederalom ackingim no thol  potobelyol wescenct in mcutill colficall who fakelbayin awnded bett hhe loffted rea\n",
      "\n",
      "\n",
      "Epoch  4620\n",
      "Train loss  0.8487072587013245\n",
      "Sample : \u001b[1ml street journal in its own grou\u001b[0msstritlerusting kealys and s tered in is was jould long it no and farrecous aigumat dogennte f he farmey was sume ve thity ht he\n",
      "\n",
      "\n",
      "Epoch  4630\n",
      "Train loss  0.410534143447876\n",
      "Sample : \u001b[1m a quiet exasperation crept into\u001b[0mk dates ffer oulded to aung it un yopresea nvar arsupllas glass anfrres inte atr toldong from chochewm hed ge t wiot lethal at u\n",
      "\n",
      "\n",
      "Epoch  4640\n",
      "Train loss  0.2904718220233917\n",
      "Sample : \u001b[1m and sex is definitely out those\u001b[0mand wealo thacolived ewothers quat is aslly of thet speming im atfer jalizile andy argitze fim raste purgets anbe bbugazr ortwai\n",
      "\n",
      "\n",
      "Epoch  4650\n",
      "Train loss  0.2542227804660797\n",
      "Sample : \u001b[1m had talked to friends who had w\u001b[0mhad talunt to mrkeng of who sousdes menthe pfroochto midatin fatr ait itho facr hirdearveny watl nhe wascous fige bus hear hawdl\n",
      "\n",
      "\n",
      "Epoch  4660\n",
      "Train loss  0.2533431053161621\n",
      "Sample : \u001b[1mat is going on here but i was he\u001b[0mt is goint on 2he iroassa hing andistingglosstousaw ones ippes bus seot tues lunge ta the creption of gliof heleadmat and staria\n",
      "\n",
      "\n",
      "Epoch  4670\n",
      "Train loss  0.3244210183620453\n",
      "Sample : \u001b[1mut the veracity of hack heaven a\u001b[0mt hit presting ios lusesmated bo guted bulw ynon walk degledn moked a reary hisheqiig coupct and incoutrrelow the tel f recaure \n",
      "\n",
      "\n",
      "Epoch  4680\n",
      "Train loss  1.296064019203186\n",
      "Sample : \u001b[1mf character and culture she was \u001b[0mst llags ond shemedines in wayh nhe the id and iphelewhin awnat us noun kid sxedors thes verethe glaps the lethered ofre seeding\n",
      "\n",
      "\n",
      "Epoch  4690\n",
      "Train loss  0.6074433922767639\n",
      "Sample : \u001b[1md himself as the darth vader of \u001b[0m theyent he andy als weyenrs live shived a tew realifessere wis of butherits had ickes le ho et plonscaped ofterid fit vembe th \n",
      "\n",
      "\n",
      "Epoch  4700\n",
      "Train loss  0.37288010120391846\n",
      "Sample : \u001b[1mt chuck i lied i wasnt at the co\u001b[0m ffock amduthes autedien ta hsceponve buother pasd sest he fondisgre haghed solier frap a canccunted thas onhens is an a blengar\n",
      "\n",
      "\n",
      "Epoch  4710\n",
      "Train loss  0.24700719118118286\n",
      "Sample : \u001b[1m of tnr seriously when it comes \u001b[0mof the seecinucht hid knt starhod int ambetfis wat tor nesthene to laveruut he was jant prede the ugstented sond it cleen anmane\n",
      "\n",
      "\n",
      "Epoch  4720\n",
      "Train loss  0.21461130678653717\n",
      "Sample : \u001b[1m from vanity fair the killers tr\u001b[0mpurm butinl ffrcop on the dew werent taay fromed aldestacl evfe snem the martipliow il te to hit vented wrohed insigusence ficen\n",
      "\n",
      "\n",
      "Epoch  4730\n",
      "Train loss  0.19801658391952515\n",
      "Sample : \u001b[1mters sloppiness he took advantag\u001b[0mers suspibut ster onster ter m af thein reaw th stichiver caskiolleardes in the warntuy jo plen glass and whics craitay destecin\n",
      "\n",
      "\n",
      "Epoch  4740\n",
      "Train loss  0.2736063599586487\n",
      "Sample : \u001b[1mhan the most sustained fraud in \u001b[0man the most sustailed oullised and insaminzirnating af whing enathrehy thal amsut in antioted itnewhoss wist him he had been aev\n",
      "\n",
      "\n",
      "Epoch  4750\n",
      "Train loss  1.4874203205108643\n",
      "Sample : \u001b[1miew for vanity fair in which he \u001b[0mely cilvente puoffis exf had haddd andt the coppliot desfrubutz checanke and lntert tho king his pare celand ntorver itw thet me\n",
      "\n",
      "\n",
      "Epoch  4760\n",
      "Train loss  0.6772902011871338\n",
      "Sample : \u001b[1mrybodys protg as michael kelly w\u001b[0mnghten porked kocbueche lywe tad wire thit is armoding hid it he wad hi kit pe in the gualil aniver his cendorting to konk oce a\n",
      "\n",
      "\n",
      "Epoch  4770\n",
      "Train loss  0.45523589849472046\n",
      "Sample : \u001b[1m chair off the ground to see if \u001b[0mfald if verion jund so wast arporges ackatadrsesurily ad as ixped sy in thet jubont in tho frement anate his aut coment arposted\n",
      "\n",
      "\n",
      "Epoch  4780\n",
      "Train loss  0.2715967893600464\n",
      "Sample : \u001b[1m the company featured in hack he\u001b[0mthe somited trothit ne in auslothe cadid s ac becker and touls youg ansed carpont ngp stntsonc dealis wey tup conf culvants a pe\n",
      "\n",
      "\n",
      "Epoch  4790\n",
      "Train loss  0.21606381237506866\n",
      "Sample : \u001b[1mterity we called some of the num\u001b[0meted to saidits the canctnemestiod on warn the res ritures shin to hearizel wry tol ancn af ghowe bectick a 1fod dly blerg astin\n",
      "\n",
      "\n",
      "Epoch  4800\n",
      "Train loss  0.20859302580356598\n",
      "Sample : \u001b[1mtil its a reality it makes you m\u001b[0mil its a becaty j 159dinher ackealat canverukglt kewhen beent ile aguress theil sespeas op to laker in the fel yorbeikn fer uh i\n",
      "\n",
      "\n",
      "Epoch  4810\n",
      "Train loss  0.23554356396198273\n",
      "Sample : \u001b[1m glasss voice he seemed to imply\u001b[0mglasss loon a me tomby soudrivele dore ward his givin gyverly as corlokt kea fichible alathingts suteiry bout andil thes atllys \n",
      "\n",
      "\n",
      "Epoch  4820\n",
      "Train loss  0.39801159501075745\n",
      "Sample : \u001b[1m testament to the feelings of so\u001b[0mtestament  ibrot onomes all sat ibougtay masten whot had a cawe thliofe toutht gear hiatidedeprripains githing lanct af an excen\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4830\n",
      "Train loss  1.3103244304656982\n",
      "Sample : \u001b[1mer actually returned but in spir\u001b[0mk petualin bechurr cemzabs foing vreak ham himples doather preqrempriz cosstlfrem sha sas afrout wher coml sked it eresy phelegt\n",
      "\n",
      "\n",
      "Epoch  4840\n",
      "Train loss  0.7083889842033386\n",
      "Sample : \u001b[1myoure my editor and you should b\u001b[0moupr siletinstivited to douke lane reveress o tow dupe the begons epolfaic to so cioc atr sughes smedives fif checa inity thay n\n",
      "\n",
      "\n",
      "Epoch  4850\n",
      "Train loss  0.3749503791332245\n",
      "Sample : \u001b[1msecure boy who would describe a \u001b[0mccurd tor who hornd desering a tary hher was nepe suchtaling anounef ant it camoncon in the vicnt rambrim a 1997 bgtt gars wtad \n",
      "\n",
      "\n",
      "Epoch  4860\n",
      "Train loss  0.25095894932746887\n",
      "Sample : \u001b[1mffice at his computer and was cl\u001b[0mmine at his cccoplacis ehess breviss owl gres in rived 1ioc si fin w beidnthits at lenit not an the incishec sem sew st atsot ly\n",
      "\n",
      "\n",
      "Epoch  4870\n",
      "Train loss  0.20748679339885712\n",
      "Sample : \u001b[1md who had seeped inside the skin\u001b[0m oly as iosedut ssther the pinded intentait gesssibuiit thas yelun his titel ima sinisen nitemion co from to bac styroust of the\n",
      "\n",
      "\n",
      "Epoch  4880\n",
      "Train loss  0.19842833280563354\n",
      "Sample : \u001b[1ms protg but in one of the incide\u001b[0m protg but in and ot desmer ind but midean shas sceerntingephens that at himpes  jok blyincons atters ar the fe douliba the  alp\n",
      "\n",
      "\n",
      "Epoch  4890\n",
      "Train loss  0.20446400344371796\n",
      "Sample : \u001b[1mories of others glass establishe\u001b[0mries of others glass estouthey wentre upsionsl tand wor soldimattephed blece ithing ore chine in athet reas swas cempes abouc to\n",
      "\n",
      "\n",
      "Epoch  4900\n",
      "Train loss  0.3532596528530121\n",
      "Sample : \u001b[1mhe wall march 24 1997 dont you d\u001b[0me wall bouch f joulle an in onal atrecritar of wakn bey bly mas bite gand he warkdgikes mhe oftore ntartic comscimbec ap  toko d\n",
      "\n",
      "\n",
      "Epoch  4910\n",
      "Train loss  1.3283624649047852\n",
      "Sample : \u001b[1mde the issue careers digital edi\u001b[0mnge dithitspustion lasklive edial s hecofsebuti lous nitereption os druacy uthe ho daverubali ghad cllled fras quat caricalo cou\n",
      "\n",
      "\n",
      "Epoch  4920\n",
      "Train loss  0.5796388983726501\n",
      "Sample : \u001b[1mrelatively small fictional detai\u001b[0mplots he ke ands a truper of wishended reme thly many leas wous mone italy indes the foon for exeding is she grasser istporsthec\n",
      "\n",
      "\n",
      "Epoch  4930\n",
      "Train loss  0.3557582497596741\n",
      "Sample : \u001b[1macting he knew exactly what he h\u001b[0mcting fe tre ic ubliz hid he kela the yebyentareated hicd bit kione wast hespers autervioven ablyot nat lisoun hadesl why proplo\n",
      "\n",
      "\n",
      "Epoch  4940\n",
      "Train loss  0.2805733382701874\n",
      "Sample : \u001b[1mcteristics namely the way many p\u001b[0mtere the vercally it 1997 laf eder thenverep alboy wrore the kestousaed yburnsunts for thes a fer saur careccusces confler wis s\n",
      "\n",
      "\n",
      "Epoch  4950\n",
      "Train loss  0.2361508458852768\n",
      "Sample : \u001b[1mtor of the magazine extra publis\u001b[0mor of in e tarenee italisslatal yelurg choppe andilated yo fres leseveng the plane thise it kexllasitieg the ffor dafint beto hi\n",
      "\n",
      "\n",
      "Epoch  4960\n",
      "Train loss  0.23268787562847137\n",
      "Sample : \u001b[1mll escaped him and he wasnt abov\u001b[0ml escauptor o thal y bitine sinhers tellory wader pobting track the shohe futlenslately iv the ugra fach ckiskis fard ablussing \n",
      "\n",
      "\n",
      "Epoch  4970\n",
      "Train loss  0.3516901135444641\n",
      "Sample : \u001b[1m his skill at creating incredibl\u001b[0mhis said the leganime to kel of dobpr and his ghons hichtre cend fuchrely andvavicossanded him rivcling ffor wes colfening an ju\n",
      "\n",
      "\n",
      "Epoch  4980\n",
      "Train loss  0.8987634181976318\n",
      "Sample : \u001b[1m expense of anything said chuck \u001b[0mrfperse cadinative far is no sconest hes a bemouscateryit and hiverons acout tik lasis vedyt if cellyfandallagglasss pentimeer t\n",
      "\n",
      "\n",
      "Epoch  4990\n",
      "Train loss  0.6693746447563171\n",
      "Sample : \u001b[1mhe former colleague people were \u001b[0me wate rem tif nand darad kelles and to das as doken on toounglapson cithchis had bot or gearsl te fadubrichingost onds uperingl\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f371020b0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 15\n",
      "  0\n",
      " 33\n",
      " 11\n",
      " 30\n",
      " 15\n",
      "  0\n",
      " 28\n",
      " 15\n",
      " 23\n",
      "  0\n",
      " 30\n",
      " 19\n",
      " 16\n",
      "  0\n",
      " 24\n",
      " 11\n",
      " 24\n",
      " 14\n",
      "  0\n",
      " 14\n",
      " 11\n",
      " 28\n",
      " 11\n",
      " 14\n",
      "  0\n",
      " 21\n",
      " 15\n",
      " 22\n",
      " 22\n",
      " 15\n",
      " 29\n",
      "  0\n",
      " 11\n",
      " 24\n",
      " 14\n",
      "  0\n",
      " 30\n",
      " 25\n",
      "  0\n",
      " 14\n",
      " 11\n",
      " 29\n",
      "  0\n",
      " 11\n",
      " 29\n",
      "  0\n",
      " 14\n",
      " 25\n",
      " 21\n",
      " 15\n",
      " 24\n",
      "  0\n",
      " 25\n",
      " 24\n",
      "  0\n",
      " 30\n",
      " 25\n",
      " 25\n",
      " 31\n",
      " 24\n",
      " 17\n",
      " 22\n",
      " 11\n",
      " 26\n",
      " 29\n",
      " 25\n",
      " 24\n",
      "  0\n",
      " 13\n",
      " 19\n",
      " 30\n",
      " 18\n",
      " 13\n",
      " 18\n",
      " 19\n",
      " 29\n",
      "  0\n",
      " 18\n",
      " 11\n",
      " 14\n",
      "  0\n",
      " 12\n",
      " 25\n",
      " 30\n",
      "  0\n",
      " 25\n",
      " 28\n",
      "  0\n",
      " 17\n",
      " 15\n",
      " 11\n",
      " 28\n",
      " 29\n",
      " 22\n",
      "  0\n",
      " 30\n",
      " 15\n",
      "  0\n",
      " 16\n",
      " 11\n",
      " 14\n",
      " 31\n",
      " 12\n",
      " 28\n",
      " 19\n",
      " 13\n",
      " 18\n",
      " 19\n",
      " 24\n",
      " 17\n",
      " 25\n",
      " 29\n",
      " 30\n",
      "  0\n",
      " 25\n",
      " 24\n",
      " 14\n",
      " 29\n",
      "  0\n",
      " 31\n",
      " 26\n",
      " 15\n",
      " 28\n",
      " 19\n",
      " 24\n",
      " 17\n",
      " 22\n",
      "[torch.cuda.LongTensor of size 128 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "init_size = 32\n",
    "\n",
    "mon_fichier = 'test_train.txt'\n",
    "data,vocab = make_files(mon_fichier,\"mon_tenseur.pt\",\"mon_vocab.pt\")\n",
    "cdset = CharDataset(\"mon_tenseur.pt\",\"mon_vocab.pt\",init_size)\n",
    "dataload = DataLoader(cdset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "n_words = len(vocab)\n",
    "\n",
    "m = Modele(256,batch_size)\n",
    "\n",
    "\n",
    "epochs=5000\n",
    "optimizer = torch.optim.SGD(m.parameters(), lr=0.01,momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "l = []\n",
    "for ep in range(epochs):\n",
    "        g_loss = 0.0\n",
    "        it = 0\n",
    "        for i,(data,target) in enumerate(dataload):\n",
    "            optimizer.zero_grad()\n",
    "            data = binarize(data, n_words)\n",
    "            x = Variable(data).cuda()\n",
    "            y = Variable(target.long()).cuda()\n",
    "            target_pred = Variable(binarize(target, n_words)).cuda()\n",
    "            pred = m.forward(x, target_pred)\n",
    "            err = loss(pred.view(-1, n_words), y.view(-1))\n",
    "            err.backward()\n",
    "            g_loss += err\n",
    "            optimizer.step()\n",
    "            it = i\n",
    "#             if ep%100 == 0:\n",
    "#                 print(x)               \n",
    "#                 t = m.forward(x,test=True,taille=3)\n",
    "#                 print(t)\n",
    "#                 print(gr)\n",
    "        l.append(g_loss[0].data)\n",
    "        if ep%10==0:\n",
    "            print(\"Epoch \", ep)\n",
    "            print(\"Train loss \", g_loss.data[0]/it)\n",
    "            start = next(iter(dataload))\n",
    "            sample = start[0][0].long()\n",
    "            data = binarize(sample, n_words)\n",
    "            x = Variable(data.unsqueeze(0)).cuda()\n",
    "            out = m.predict(x,128)\n",
    "            o = code2char(out.view(-1),vocab)\n",
    "#             print('PREDICT : ', o)\n",
    "            BOLD = '\\033[1m'\n",
    "            END = '\\033[0m'\n",
    "            print(\"Sample : \"+BOLD+code2char(sample.view(-1), vocab)+END+code2char(out.view(-1), vocab))\n",
    "            print('\\n')\n",
    "    \n",
    "plt.plot(range(epochs),l)\n",
    "plt.show()\n",
    "               \n",
    "    \n",
    "\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
