{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charDataset import *\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train = torch.load('train_data.tx')\n",
    "vocab = torch.load('vocab.tx')\n",
    "n_words = len(vocab)\n",
    "\n",
    "\n",
    "#c = CharDataset('train_data.tx','vocab.tx',50)\n",
    "\n",
    "#retourne une sequence dans [0] et le reste dans [1]\n",
    "def seq(text,debut,fin):\n",
    "    return text[debut:fin], text[fin:]\n",
    "\n",
    "# retourne une sequence codé dans [0] et le reste dans [1]\n",
    "def code_seq(text,debut,fin,vocab):\n",
    "    s = seq(text,debut,fin)\n",
    "    return char2code(s[0],vocab), char2code(s[1],vocab)\n",
    "\n",
    "text = 'je suis la'\n",
    "\n",
    "\n",
    "def transform_one_hot(digit,n):\n",
    "    y_onehot = torch.FloatTensor(n)\n",
    "    y_onehot.zero_()\n",
    "    y_onehot[digit] = 1\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "def transform_one_hot_sequence(sequence,n):\n",
    "    seq_onehot = torch.FloatTensor(len(sequence),n)\n",
    "    for i in range(len(sequence)):\n",
    "        seq_onehot[i] = transform_one_hot(sequence[i],n)\n",
    "    return seq_onehot\n",
    "\n",
    "#transform_one_hot_sequence(char2code('n vreb br',vocab),n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je n'ai pas réussi a codé le RNN. Dans le forward, j'obtiens une somme des proba différentes de 1 mais je n'arrive pas à comprendre pourquoi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modele(nn.Module):\n",
    "    def __init__(self,l):\n",
    "        super(Modele,self).__init__()\n",
    "        self.l = l\n",
    "        \n",
    "        #init h a 0 de dimension d a changer a chaque linear donc 2 param a apprendre w_h et w_x\n",
    "        # Il faut 3 linear: \n",
    "        # - un pour passer de Rv -> Rd\n",
    "        # le 1er converti une lettre (dimension v) dans l'espace latent\n",
    "        # - un pour passer de Rd -> Rd\n",
    "        # le 2e predit la suite dans l'espace latent\n",
    "        # - un pour passer de Rd -> Rv\n",
    "        # le 3e transforme dans v\n",
    "        # on doit faire un logSoftmax pour avoir p(y1 | x_0,h_0) puis tirage puis one_hot\n",
    "        self.encodage = nn.Linear(n_words,self.l, bias=False)\n",
    "        self.decodage = nn.Linear(self.l,self.l, bias=False)\n",
    "        self.d2 = nn.Linear(self.l, n_words, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward_train(self,x,y, stopWord = \"~\", test= False, size_max = 140):\n",
    "        #mettre formule du forward \n",
    "        # ne marche pas au niveau du decodage je sais pas pourquoi\n",
    "        H = [Variable(torch.zeros(x.size(0),self.l))]\n",
    "        logSoftmax = nn.LogSoftmax()\n",
    "        _, argmax = x[:,0].max(dim=-1)\n",
    "        predictions = [argmax]\n",
    "        probPrediction = []\n",
    "        size = 1\n",
    "        out = [x]\n",
    "        size_max = size_max if test else len(y)\n",
    "        while(size<size_max and predictions[-1].data[0] != stopWord):\n",
    "            h = self.tanh(self.encodage(x[-1]) + self.decodage(H[-1]))\n",
    "            H.append(h)\n",
    "            #print('somme : ' , torch.sum(h))\n",
    "            pred = self.d2(h)\n",
    "            probPrediction.append(logSoftmax(pred))\n",
    "            # somme non egale à 1\n",
    "            # un peu étrange mais je ne comprend pas pourquoi\n",
    "            #print('somme : ' , torch.sum(logSoftmax(predProba)))\n",
    "            if test : \n",
    "                out.append(probPrediction)\n",
    "                predictions.append(self.tirage(probPrediction))\n",
    "            else:\n",
    "                out.append(y)\n",
    "            size += 1\n",
    "#         print(probPrediction)\n",
    "#         print(predictions)\n",
    "        return torch.cat(probPrediction),torch.cat(predictions)\n",
    "    \n",
    "    def tirage(self, distribution):\n",
    "        distrib = torch.cat([r / r.sum(dim=-1) for r in distribution])\n",
    "        return distrib.multinomial(1)\n",
    "\n",
    "    def predict(self,x):\n",
    "        _,decoded = self.forward(x).max(dim=1)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "m = Modele(2)\n",
    "text = 'je suis la'\n",
    "test = code_seq(text,0,3,vocab)\n",
    "x_train = Variable(transform_one_hot_sequence(test[0],n_words))\n",
    "y_train = Variable(transform_one_hot_sequence(test[1],n_words))\n",
    "\n",
    "a,b = m.forward_train(x_train,y_train)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
